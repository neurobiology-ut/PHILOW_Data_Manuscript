{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf0047",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]= \"true\"\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam , RMSprop\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint , EarlyStopping , CSVLogger\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from skimage import morphology\n",
    "from scipy.ndimage import label as LABEL\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01e7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#値を-1から1に正規化する関数\n",
    "def normalize_x(image):\n",
    "    return image / 127.5 - 1\n",
    "\n",
    "\n",
    "#値を0から1正規化する関数\n",
    "def normalize_y(image):\n",
    "    return image / 255\n",
    "\n",
    "\n",
    "#値を0から255に戻す関数\n",
    "def denormalize_y(image):\n",
    "    return image * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61963c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インプット画像を読み込む関数\n",
    "def load_X_gray(folder_path):\n",
    "    \n",
    "    image_files = []\n",
    "\n",
    "    #image_files = os.listdir(folder_path)\n",
    "       \n",
    "    for file in os.listdir(folder_path):\n",
    "        base, ext = os.path.splitext(file)\n",
    "        if ext == '.png':\n",
    "            image_files.append(file)\n",
    "        else :\n",
    "            pass\n",
    "        \n",
    "    image_files.sort()\n",
    "    \n",
    "    img = cv2.imread(folder_path + os.sep + image_files[0], cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    #image_files = image_files[1:]\n",
    "    images = np.zeros((len(image_files), img.shape[0], img.shape[1], 1), np.float32)\n",
    "    for i, image_file in tqdm(enumerate(image_files)):\n",
    "        image = cv2.imread(folder_path + os.sep + image_file, cv2.IMREAD_GRAYSCALE)\n",
    "        #image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        image = image[:, :, np.newaxis]\n",
    "        images[i] = normalize_x(image)\n",
    "    \n",
    "    print(images.shape)\n",
    "    \n",
    "    return images, image_files\n",
    "\n",
    "\n",
    "def load_Y_gray(folder_path, thresh = None , normalize = False, g_size = None):\n",
    "    image_files = []\n",
    "    #image_files = os.listdir(folder_path)\n",
    "    \n",
    "    for file in os.listdir(folder_path):\n",
    "        base, ext = os.path.splitext(file)\n",
    "        if ext == '.png':\n",
    "            image_files.append(file)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    image_files.sort()\n",
    "    \n",
    "    img = cv2.imread(folder_path + os.sep + image_files[0], cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    images = np.zeros(\n",
    "        (len(image_files), img.shape[0], img.shape[1], 1) ,np.float32\n",
    "    )\n",
    "    \n",
    "    for i , image_file in tqdm(enumerate(image_files)):\n",
    "        image = cv2.imread(\n",
    "            folder_path + os.sep + image_file ,\n",
    "            cv2.IMREAD_GRAYSCALE\n",
    "        )\n",
    "        #print(image.shape)\n",
    "        \n",
    "        # ぼかし処理\n",
    "        if g_size:\n",
    "            image = cv2.GaussianBlur(\n",
    "                image, (g_size, g_size), 0\n",
    "            )        \n",
    "        \n",
    "        if thresh:\n",
    "            ret , image = cv2.threshold(image , thresh , 255 , cv2.THRESH_BINARY)\n",
    "        image = image[ : , : , np.newaxis]\n",
    "        if normalize:\n",
    "            images[i] = normalize_y(image)\n",
    "        else:\n",
    "            images[i] = image\n",
    "            \n",
    "    print(images.shape)\n",
    "    \n",
    "    return images , image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb34d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def weighted_dice_coeff(y_true, y_pred, weight):\n",
    "    smooth = 1.\n",
    "    w, m1, m2 = weight * weight, y_true, y_pred\n",
    "    intersection = (m1 * m2)\n",
    "    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n",
    "    return score\n",
    "\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd number\n",
    "    if K.int_shape(y_pred)[1] == 128:\n",
    "        kernel_size = 11\n",
    "    elif K.int_shape(y_pred)[1] == 256:\n",
    "        kernel_size = 21\n",
    "    elif K.int_shape(y_pred)[1] == 512:\n",
    "        kernel_size = 21\n",
    "    elif K.int_shape(y_pred)[1] == 1024:\n",
    "        kernel_size = 41\n",
    "    else:\n",
    "        raise ValueError('Unexpected image size')\n",
    "    averaged_mask = K.pool2d(\n",
    "        y_true, pool_size=(kernel_size, kernel_size), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    border = K.cast(K.greater(averaged_mask, 0.005), 'float32') * K.cast(K.less(averaged_mask, 0.995), 'float32')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight += border * 2\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = 1 - weighted_dice_coeff(y_true, y_pred, weight)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight):\n",
    "    # avoiding overflow\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits\n",
    "    loss = (1. - y_true) * logit_y_pred + (1. + (weight - 1.) * y_true) * \\\n",
    "                                          (K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n",
    "    return K.sum(loss) / K.sum(weight)\n",
    "\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd number\n",
    "    if K.int_shape(y_pred)[1] == 128:\n",
    "        kernel_size = 11\n",
    "    elif K.int_shape(y_pred)[1] == 256:\n",
    "        kernel_size = 21\n",
    "    elif K.int_shape(y_pred)[1] == 512:\n",
    "        kernel_size = 21\n",
    "    elif K.int_shape(y_pred)[1] == 1024:\n",
    "        kernel_size = 41\n",
    "    else:\n",
    "        raise ValueError('Unexpected image size')\n",
    "    averaged_mask = K.pool2d(\n",
    "        y_true, pool_size=(kernel_size, kernel_size), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    border = K.cast(K.greater(averaged_mask, 0.005), 'float32') * K.cast(K.less(averaged_mask, 0.995), 'float32')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight += border * 2\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = weighted_bce_loss(y_true, y_pred, weight) + (1 - weighted_dice_coeff(y_true, y_pred, weight))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52431813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet_resnet_512(input_shape=(512, 512, 1), num_classes=3):\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    res0 = Conv2D(16, (1, 1), padding = 'same',  use_bias = False)(inputs)\n",
    "    down0 = Conv2D(16, (3, 3), padding='same')(inputs)\n",
    "    down0 = BatchNormalization()(down0)\n",
    "    down0 = Activation('relu')(down0)\n",
    "    down0 = Conv2D(16, (3, 3), padding='same')(down0)\n",
    "    down0 = BatchNormalization()(down0)\n",
    "    down0 = Add()([res0, down0])\n",
    "    down0 = Activation('relu')(down0)\n",
    "    down0_pool = MaxPooling2D((2, 2), strides=(2, 2))(down0)\n",
    "    \n",
    "    res1 = Conv2D(32, (1, 1), padding = 'same',  use_bias = False)(down0_pool)\n",
    "    down1 = Conv2D(32, (3, 3), padding='same')(down0_pool)\n",
    "    down1 = BatchNormalization()(down1)\n",
    "    down1 = Activation('relu')(down1)\n",
    "    down1 = Conv2D(32, (3, 3), padding='same')(down1)\n",
    "    down1 = BatchNormalization()(down1)\n",
    "    down1 = Add()([res1, down1])\n",
    "    down1 = Activation('relu')(down1)\n",
    "    down1_pool = MaxPooling2D((2, 2), strides=(2, 2))(down1)\n",
    "    \n",
    "    res2 = Conv2D(64, (1, 1), padding = 'same',  use_bias = False)(down1_pool)\n",
    "    down2 = Conv2D(64, (3, 3), padding='same')(down1_pool)\n",
    "    down2 = BatchNormalization()(down2)\n",
    "    down2 = Activation('relu')(down2)\n",
    "    down2 = Conv2D(64, (3, 3), padding='same')(down2)\n",
    "    down2 = BatchNormalization()(down2)\n",
    "    down2 = Add()([res2, down2])\n",
    "    down2 = Activation('relu')(down2)\n",
    "    down2_pool = MaxPooling2D((2, 2), strides=(2, 2))(down2)\n",
    "    \n",
    "    res3 = Conv2D(128, (1, 1), padding = 'same',  use_bias = False)(down2_pool)\n",
    "    down3 = Conv2D(128, (3, 3), padding='same')(down2_pool)\n",
    "    down3 = BatchNormalization()(down3)\n",
    "    down3 = Activation('relu')(down3)\n",
    "    down3 = Conv2D(128, (3, 3), padding='same')(down3)\n",
    "    down3 = BatchNormalization()(down3)\n",
    "    down3 = Add()([res3, down3])\n",
    "    down3 = Activation('relu')(down3)\n",
    "    down3_pool = MaxPooling2D((2, 2), strides=(2, 2))(down3)\n",
    "    \n",
    "    res4 = Conv2D(256, (1, 1), padding = 'same',  use_bias = False)(down3_pool)\n",
    "    down4 = Conv2D(256, (3, 3), padding='same')(down3_pool)\n",
    "    down4 = BatchNormalization()(down4)\n",
    "    down4 = Activation('relu')(down4)\n",
    "    down4 = Conv2D(256, (3, 3), padding='same')(down4)\n",
    "    down4 = BatchNormalization()(down4)\n",
    "    down4 = Add()([res4, down4])\n",
    "    down4 = Activation('relu')(down4)\n",
    "    down4_pool = MaxPooling2D((2, 2), strides=(2, 2))(down4)\n",
    "    \n",
    "    res5 = Conv2D(512, (1, 1), padding = 'same',  use_bias = False)(down4_pool)\n",
    "    down5 = Conv2D(512, (3, 3), padding='same')(down4_pool)\n",
    "    down5 = BatchNormalization()(down5)\n",
    "    down5 = Activation('relu')(down5)\n",
    "    down5 = Conv2D(512, (3, 3), padding='same')(down5)\n",
    "    down5 = BatchNormalization()(down5)\n",
    "    down5 = Add()([res5, down5])\n",
    "    down5 = Activation('relu')(down5)\n",
    "    down5_pool = MaxPooling2D((2, 2), strides=(2, 2))(down5)\n",
    "    \n",
    "    res6 = Conv2D(1024, (1, 1), padding = 'same',  use_bias = False)(down5_pool)\n",
    "    center = Conv2D(1024, (3, 3), padding='same')(down5_pool)\n",
    "    center = BatchNormalization()(center)\n",
    "    center = Activation('relu')(center)\n",
    "    center = Conv2D(1024, (3, 3), padding='same')(center)\n",
    "    center = BatchNormalization()(center)\n",
    "    center = Add()([res6, center])\n",
    "    center = Activation('relu')(center)\n",
    "    \n",
    "    up5 = UpSampling2D((2, 2))(center)\n",
    "    up5 = Conv2D(512, (2, 2), padding = 'same')(up5)\n",
    "    up5 = Activation('relu')(up5)\n",
    "    up5 = concatenate([up5, down5], axis = 3)\n",
    "    res_up5 = Conv2D(512, (1, 1), padding = 'same', use_bias = False)(up5)\n",
    "    up5 = Conv2D(512, (3, 3), padding = 'same')(up5)\n",
    "    up5 = BatchNormalization()(up5)\n",
    "    up5 = Activation('relu')(up5)\n",
    "    up5 = Conv2D(512, (3, 3), padding = 'same')(up5)\n",
    "    up5 = BatchNormalization()(up5)\n",
    "    up5 = Add()([res_up5, up5])\n",
    "    up5 = Activation('relu')(up5)\n",
    "    \n",
    "    up4 = UpSampling2D((2, 2))(up5)\n",
    "    up4 = Conv2D(256, (2, 2), padding = 'same')(up4)\n",
    "    up4 = Activation('relu')(up4)\n",
    "    up4 = concatenate([up4, down4], axis = 3)\n",
    "    res_up4 = Conv2D(256, (1, 1), padding = 'same', use_bias = False)(up4)\n",
    "    up4 = Conv2D(256, (3, 3), padding = 'same')(up4)\n",
    "    up4 = BatchNormalization()(up4)\n",
    "    up4 = Activation('relu')(up4)\n",
    "    up4 = Conv2D(256, (3, 3), padding = 'same')(up4)\n",
    "    up4 = BatchNormalization()(up4)\n",
    "    up4 = Add()([res_up4, up4])\n",
    "    up4 = Activation('relu')(up4)\n",
    "    \n",
    "    up3 = UpSampling2D((2, 2))(up4)\n",
    "    up3 = Conv2D(128, (2, 2), padding = 'same')(up3)\n",
    "    up3 = Activation('relu')(up3)\n",
    "    up3 = concatenate([up3, down3], axis = 3)\n",
    "    res_up3 = Conv2D(128, (1, 1), padding = 'same', use_bias = False)(up3)\n",
    "    up3 = Conv2D(128, (3, 3), padding = 'same')(up3)\n",
    "    up3 = BatchNormalization()(up3)\n",
    "    up3 = Activation('relu')(up3)\n",
    "    up3 = Conv2D(128, (3, 3), padding = 'same')(up3)\n",
    "    up3 = BatchNormalization()(up3)\n",
    "    up3 = Add()([res_up3, up3])\n",
    "    up3 = Activation('relu')(up3)\n",
    "    \n",
    "    up2 = UpSampling2D((2, 2))(up3)\n",
    "    up2 = Conv2D(64, (2, 2), padding = 'same')(up2)\n",
    "    up2 = Activation('relu')(up2)\n",
    "    up2 = concatenate([up2, down2], axis = 3)\n",
    "    res_up2 = Conv2D(64, (1, 1), padding = 'same', use_bias = False)(up2)\n",
    "    up2 = Conv2D(64, (3, 3), padding = 'same')(up2)\n",
    "    up2 = BatchNormalization()(up2)\n",
    "    up2 = Activation('relu')(up2)\n",
    "    up2 = Conv2D(64, (3, 3), padding = 'same')(up2)\n",
    "    up2 = BatchNormalization()(up2)\n",
    "    up2 = Add()([res_up2, up2])\n",
    "    up2 = Activation('relu')(up2)\n",
    "    \n",
    "    up1 = UpSampling2D((2, 2))(up2)\n",
    "    up1 = Conv2D(32, (2, 2), padding = 'same')(up1)\n",
    "    up1 = Activation('relu')(up1)\n",
    "    up1 = concatenate([up1, down1], axis = 3)\n",
    "    res_up1 = Conv2D(32, (1, 1), padding = 'same', use_bias = False)(up1)\n",
    "    up1 = Conv2D(32, (3, 3), padding = 'same')(up1)\n",
    "    up1 = BatchNormalization()(up1)\n",
    "    up1 = Activation('relu')(up1)\n",
    "    up1 = Conv2D(32, (3, 3), padding = 'same')(up1)\n",
    "    up1 = BatchNormalization()(up1)\n",
    "    up1 = Add()([res_up1, up1])\n",
    "    up1 = Activation('relu')(up1)\n",
    "    \n",
    "    up0 = UpSampling2D((2, 2))(up1)\n",
    "    up0 = Conv2D(16, (2, 2), padding = 'same')(up0)\n",
    "    up0 = Activation('relu')(up0)\n",
    "    up0 = concatenate([up0, down0], axis = 3)\n",
    "    res_up0 = Conv2D(16, (1, 1), padding = 'same', use_bias = False)(up0)\n",
    "    up0 = Conv2D(16, (3, 3), padding = 'same')(up0)\n",
    "    up0 = BatchNormalization()(up0)\n",
    "    up0 = Activation('relu')(up0)\n",
    "    up0 = Conv2D(16, (3, 3), padding = 'same')(up0)\n",
    "    up0 = BatchNormalization()(up0)\n",
    "    up0 = Add()([res_up0, up0])\n",
    "    up0 = Activation('relu')(up0)\n",
    "    \n",
    "    classify = Conv2D(num_classes, (1, 1), activation='sigmoid')(up0)\n",
    "    #classify = Conv2D(num_classes, (1, 1), padding = 'same', activation='softmax')(up0)\n",
    "    model = Model(inputs=inputs, outputs=classify)\n",
    "    \n",
    "    model.compile(optimizer=RMSprop(lr=0.0001), loss=bce_dice_loss, metrics=[dice_coeff])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b361e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_rgb_imgs(images):\n",
    "    \n",
    "    '''\n",
    "        512 x 512に分割する\n",
    "        端の50pxをのりしろとする\n",
    "        空欄箇所は-1（黒）にする\n",
    "        height, widthが412未満の場合は変数を-1に変更\n",
    "    '''\n",
    "    \n",
    "    H = -(-images.shape[1]//412)\n",
    "    W = -(-images.shape[2]//412)\n",
    "    \n",
    "    diveded_imgs = np.zeros(( images.shape[0]*H*W, 512, 512, 1), np.float32)\n",
    "    print(H,W)\n",
    "    \n",
    "    for z in range(images.shape[0]):\n",
    "        image = images[z]\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "                cropped_img = np.zeros((512, 512, 1), np.float32)\n",
    "                #cropped_img -= 1\n",
    "                \n",
    "                if images.shape[1] < 412:\n",
    "                    h = -1\n",
    "                if images.shape[2] < 412:\n",
    "                    w = -1\n",
    "                    \n",
    "                if h == -1:\n",
    "                    if w == -1:\n",
    "                        cropped_img[50:images.shape[1]+50, 50:images.shape[2]+50, 0] = image[0:images.shape[1], 0:images.shape[2], 0]\n",
    "                    elif w == 0:\n",
    "                        cropped_img[50:images.shape[1]+50, 50:512, 0] = image[0:images.shape[1], 0:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        cropped_img[50:images.shape[1]+50, 0:images.shape[2]-412*W-50, 0] = image[0:images.shape[1], w*412-50:images.shape[2], 0]\n",
    "                    else:\n",
    "                        cropped_img[50:images.shape[1]+50, :, 0] = image[0:images.shape[1], w*412-50:(w+1)*412+50, 0]\n",
    "                elif h == 0:\n",
    "                    if w == -1:\n",
    "                        cropped_img[50:512, 50:images.shape[2]+50, 0] = image[0:462, 0:images.shape[2], 0]\n",
    "                    elif w == 0:\n",
    "                        cropped_img[50:512, 50:512, 0] = image[0:462, 0:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        cropped_img[50:512, 0:images.shape[2]-412*W-50, 0] = image[0:462, w*412-50:images.shape[2], 0]\n",
    "                    else:\n",
    "                        #cropped_img[50:512, :, 0] = image[0:462, w*412-50:(w+1)*412+50, 0]\n",
    "                        try:\n",
    "                            cropped_img[50:512, :, 0] = image[0:462, w*412-50:(w+1)*412+50, 0]\n",
    "                        except:\n",
    "                            cropped_img[50:512, 0:images.shape[2]-412*(W-1)-50, 0] = image[0:462, w*412-50:(w+1)*412+50, 0]\n",
    "                elif h == H-1:\n",
    "                    if w == -1:\n",
    "                        cropped_img[0:images.shape[1]-412*H-50, 50:images.shape[2]+50, 0] = image[h*412-50:images.shape[1], 0:images.shape[2], 0]\n",
    "                    elif w == 0:\n",
    "                        cropped_img[0:images.shape[1]-412*H-50, 50:512, 0] = image[h*412-50:images.shape[1], 0:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        cropped_img[0:images.shape[1]-412*H-50, 0:images.shape[2]-412*W-50, 0] = image[h*412-50:images.shape[1], w*412-50:images.shape[2], 0]\n",
    "                    else:\n",
    "                        try:\n",
    "                            cropped_img[0:images.shape[1]-412*H-50, :, 0] = image[h*412-50:images.shape[1], w*412-50:(w+1)*412+50, 0]\n",
    "                        except:\n",
    "                            cropped_img[0:images.shape[1]-412*H-50, 0:images.shape[2]-412*(W-1)-50, 0] = image[h*412-50:images.shape[1], w*412-50:(w+1)*412+50, 0]\n",
    "                else:\n",
    "                    if w == -1:\n",
    "                        cropped_img[:, 50:images.shape[2]+50, 0] = image[h*412-50:(h+1)*412+50, 0:images.shape[2], 0]\n",
    "                    elif w == 0:\n",
    "                        #cropped_img[:, 50:512, 0] = image[h*412-50:(h+1)*412+50, 0:462, 0]\n",
    "                        try:\n",
    "                            cropped_img[:, 50:512, 0] = image[h*412-50:(h+1)*412+50, 0:462, 0]\n",
    "                        except:\n",
    "                            cropped_img[0:images.shape[1]-412*H-50+412, 50:512, 0] = image[h*412-50:(h+1)*412+50, 0:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        #cropped_img[:, 0:images.shape[2]-412*W-50, 0] = image[h*412-50:(h+1)*412+50, w*412-50:images.shape[2], 0]\n",
    "                        try:\n",
    "                            cropped_img[:, 0:images.shape[2]-412*W-50, 0] = image[h*412-50:(h+1)*412+50, w*412-50:images.shape[2], 0]\n",
    "                        except:\n",
    "                            cropped_img[0:images.shape[1]-412*H-50+412, 0:images.shape[2]-412*W-50, 0] = image[h*412-50:(h+1)*412+50, w*412-50:images.shape[2], 0]\n",
    "                    else:\n",
    "                        #cropped_img[:, :, 0] = image[h*412-50:(h+1)*412+50, w*412-50:(w+1)*412+50, 0]\n",
    "                        try:\n",
    "                            cropped_img[:, :, 0] = image[h*412-50:(h+1)*412+50, w*412-50:(w+1)*412+50, 0]         \n",
    "                        except:\n",
    "                            try:\n",
    "                                 cropped_img[:, 0:images.shape[2]-412*(W-1)-50, 0] = image[h*412-50:(h+1)*412+50, w*412-50:(w+1)*412+50, 0]\n",
    "                            except:\n",
    "                                 cropped_img[0:images.shape[1]-412*(H-1)-50, :, 0] = image[h*412-50:(h+1)*412+50, w*412-50:(w+1)*412+50, 0]\n",
    "                h = max(0, h)\n",
    "                w = max(0, w)\n",
    "                diveded_imgs[z*H*W+ w*H+h] = cropped_img\n",
    "                #print(z*H*W+ w*H+h)\n",
    "                \n",
    "    return diveded_imgs\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def divide_gray_imgs(images):\n",
    "    \n",
    "    '''\n",
    "        512 x 512に分割する\n",
    "        端の50pxをのりしろとする\n",
    "        空欄箇所は-1（黒）にする\n",
    "        height, widthが412未満の場合は変数を-1に変更\n",
    "    '''\n",
    "    \n",
    "    H = -(-images.shape[1]//412)\n",
    "    W = -(-images.shape[2]//412)\n",
    "    \n",
    "    diveded_imgs = np.zeros(( images.shape[0]*H*W, 512, 512, 1), np.float32)\n",
    "    print(H,W)\n",
    "    \n",
    "    for z in range(images.shape[0]):\n",
    "        image = images[z]\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "                cropped_img = np.zeros((512, 512, 1), np.float32)\n",
    "                cropped_img -= 1\n",
    "                \n",
    "                if images.shape[1] < 412:\n",
    "                    h = -1\n",
    "                if images.shape[2] < 412:\n",
    "                    w = -1\n",
    "                    \n",
    "                if h == -1:\n",
    "                    if w == -1:\n",
    "                        cropped_img[50:images.shape[1]+50, 50:images.shape[2]+50, 0] = image[0:images.shape[1], 0:images.shape[2], 0]\n",
    "                    elif w == 0:\n",
    "                        cropped_img[50:images.shape[1]+50, 50:512, 0] = image[0:images.shape[1], 0:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        cropped_img[50:images.shape[1]+50, 0:images.shape[2]-412*W-50, 0] = image[0:images.shape[1], w*412-50:images.shape[2], 0]\n",
    "                    else:\n",
    "                        cropped_img[50:images.shape[1]+50, :, 0] = image[0:images.shape[1], w*412-50:(w+1)*412+50, 0]\n",
    "                elif h == 0:\n",
    "                    if w == -1:\n",
    "                        cropped_img[50:512, 50:images.shape[2]+50, 0] = image[0:462, 0:images.shape[2], 0]\n",
    "                    elif w == 0:\n",
    "                        cropped_img[50:512, 50:512, 0] = image[0:462, 0:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        cropped_img[50:512, 0:images.shape[2]-412*W-50, 0] = image[0:462, w*412-50:images.shape[2], 0]\n",
    "                    else:\n",
    "                        #cropped_img[50:512, :, 0] = image[0:462, w*412-50:(w+1)*412+50, 0]\n",
    "                        try:\n",
    "                            cropped_img[50:512, :, 0] = image[0:462, w*412-50:(w+1)*412+50, 0]\n",
    "                        except:\n",
    "                            cropped_img[50:512, 0:images.shape[2]-412*(W-1)-50, 0] = image[0:462, w*412-50:(w+1)*412+50, 0]\n",
    "                elif h == H-1:\n",
    "                    if w == -1:\n",
    "                        cropped_img[0:images.shape[1]-412*H-50, 50:images.shape[2]+50, 0] = image[h*412-50:images.shape[1], 0:images.shape[2], 0]\n",
    "                    elif w == 0:\n",
    "                        cropped_img[0:images.shape[1]-412*H-50, 50:512, 0] = image[h*412-50:images.shape[1], 0:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        cropped_img[0:images.shape[1]-412*H-50, 0:images.shape[2]-412*W-50, 0] = image[h*412-50:images.shape[1], w*412-50:images.shape[2], 0]\n",
    "                    else:\n",
    "                        try:\n",
    "                            cropped_img[0:images.shape[1]-412*H-50, :, 0] = image[h*412-50:images.shape[1], w*412-50:(w+1)*412+50, 0]\n",
    "                        except:\n",
    "                            cropped_img[0:images.shape[1]-412*H-50, 0:images.shape[2]-412*(W-1)-50, 0] = image[h*412-50:images.shape[1], w*412-50:(w+1)*412+50, 0]\n",
    "                else:\n",
    "                    if w == -1:\n",
    "                        cropped_img[:, 50:images.shape[2]+50, 0] = image[h*412-50:(h+1)*412+50, 0:images.shape[2], 0]\n",
    "                    elif w == 0:\n",
    "                        #cropped_img[:, 50:512, 0] = image[h*412-50:(h+1)*412+50, 0:462, 0]\n",
    "                        try:\n",
    "                            cropped_img[:, 50:512, 0] = image[h*412-50:(h+1)*412+50, 0:462, 0]\n",
    "                        except:\n",
    "                            cropped_img[0:images.shape[1]-412*H-50+412, 50:512, 0] = image[h*412-50:(h+1)*412+50, 0:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        #cropped_img[:, 0:images.shape[2]-412*W-50, 0] = image[h*412-50:(h+1)*412+50, w*412-50:images.shape[2], 0]\n",
    "                        try:\n",
    "                            cropped_img[:, 0:images.shape[2]-412*W-50, 0] = image[h*412-50:(h+1)*412+50, w*412-50:images.shape[2], 0]\n",
    "                        except:\n",
    "                            cropped_img[0:images.shape[1]-412*H-50+412, 0:images.shape[2]-412*W-50, 0] = image[h*412-50:(h+1)*412+50, w*412-50:images.shape[2], 0]\n",
    "                    else:\n",
    "                        #cropped_img[:, :, 0] = image[h*412-50:(h+1)*412+50, w*412-50:(w+1)*412+50, 0]\n",
    "                        try:\n",
    "                            cropped_img[:, :, 0] = image[h*412-50:(h+1)*412+50, w*412-50:(w+1)*412+50, 0]         \n",
    "                        except:\n",
    "                            try:\n",
    "                                 cropped_img[:, 0:images.shape[2]-412*(W-1)-50, 0] = image[h*412-50:(h+1)*412+50, w*412-50:(w+1)*412+50, 0]\n",
    "                            except:\n",
    "                                 cropped_img[0:images.shape[1]-412*(H-1)-50, :, 0] = image[h*412-50:(h+1)*412+50, w*412-50:(w+1)*412+50, 0]\n",
    "                h = max(0, h)\n",
    "                w = max(0, w)\n",
    "                diveded_imgs[z*H*W+ w*H+h] = cropped_img\n",
    "                #print(z*H*W+ w*H+h)\n",
    "                \n",
    "    return diveded_imgs\n",
    "\n",
    "\n",
    "def divide_imgs(images):\n",
    "    if images.shape[3] == 1:\n",
    "        return divide_gray_imgs(images)\n",
    "    else:\n",
    "        return np.concatenate( list( divide_rgb_imgs(images[:,:,:,c][..., np.newaxis]) for c in range(images.shape[3]) ), axis = 3 )\n",
    "            \n",
    "    \n",
    "    \n",
    "def merge_imgs(imgs, original_image_shape):\n",
    "    \n",
    "    '''\n",
    "        元のサイズに合体させる\n",
    "        original_image_shapeはoriginal_image.shapeで与えられる、(z, h, w, channel)のタプル型\n",
    "    '''\n",
    "    \n",
    "    merged_imgs = np.zeros((original_image_shape[0], original_image_shape[1], original_image_shape[2], 1), np.float32)\n",
    "    H = -(-original_image_shape[1]//412)\n",
    "    W = -(-original_image_shape[2]//412)    \n",
    "    \n",
    "    for z in range(original_image_shape[0]):\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "\n",
    "                if original_image_shape[1] < 412:\n",
    "                    h = -1\n",
    "                if original_image_shape[2] < 412:\n",
    "                    w = -1\n",
    "                    \n",
    "                #print(z*H*W+ max(w, 0)*H+max(h, 0))    \n",
    "                if h == -1:\n",
    "                    if w == -1:\n",
    "                        merged_imgs[z, 0:original_image_shape[1], 0:original_image_shape[2], 0] = imgs[z*H*W+ 0*H+0][50:original_image_shape[1]+50, 50:original_image_shape[2]+50, 0]\n",
    "                    elif w == 0:\n",
    "                        merged_imgs[z, 0:original_image_shape[1], 0:412, 0] = imgs[z*H*W+ w*H+0][50:original_image_shape[1]+50, 50:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        merged_imgs[z, 0:original_image_shape[1], w*412:original_image_shape[2], 0] = imgs[z*H*W+ w*H+0][50:original_image_shape[1]+50, 50:original_image_shape[2]-412*W-50, 0]\n",
    "                    else:\n",
    "                        merged_imgs[z, 0:original_image_shape[1], w*412:(w+1)*412, 0] = imgs[z*H*W+ w*H+0][50:original_image_shape[1]+50, 50:462, 0]\n",
    "                elif h == 0:\n",
    "                    if w == -1:\n",
    "                        merged_imgs[z, 0:412, 0:original_image_shape[2], 0] = imgs[z*H*W+ 0*H+h][50:462, 50:original_image_shape[2]+50, 0]\n",
    "                    elif w == 0:\n",
    "                        merged_imgs[z, 0:412, 0:412, 0] = imgs[z*H*W+ w*H+h][50:462, 50:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        merged_imgs[z, 0:412, w*412:original_image_shape[2], 0] = imgs[z*H*W+ w*H+h][50:462, 50:original_image_shape[2]-412*W-50, 0]\n",
    "                    else:\n",
    "                        merged_imgs[z, 0:412, w*412:(w+1)*412, 0] = imgs[z*H*W+ w*H+h][50:462, 50:462, 0]\n",
    "                elif h == H-1:\n",
    "                    if w == -1:\n",
    "                         merged_imgs[z, h*412:original_image_shape[1], 0:original_image_shape[2], 0] = imgs[z*H*W+ 0*H+h][50:original_image_shape[1]-412*H-50, 50:original_image_shape[2]+50, 0]\n",
    "                    elif w == 0:\n",
    "                        merged_imgs[z, h*412:original_image_shape[1], 0:412, 0] = imgs[z*H*W+ w*H+h][50:original_image_shape[1]-412*H-50, 50:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        merged_imgs[z, h*412:original_image_shape[1], w*412:original_image_shape[2], 0] = imgs[z*H*W+ w*H+h][50:original_image_shape[1]-412*H-50, 50:original_image_shape[2]-412*W-50, 0]\n",
    "                    else:\n",
    "                        merged_imgs[z, h*412:original_image_shape[1], w*412:(w+1)*412, 0] = imgs[z*H*W+ w*H+h][50:original_image_shape[1]-412*H-50, 50:462, 0]\n",
    "                else:\n",
    "                    if w == -1:\n",
    "                         merged_imgs[z, h*412:(h+1)*412, 0:original_image_shape[2], 0] = imgs[z*H*W+ 0*H+h][50:462, 50:original_image_shape[2]+50, 0]\n",
    "                    elif w == 0:\n",
    "                        merged_imgs[z, h*412:(h+1)*412, 0:412, 0] = imgs[z*H*W+ w*H+h][50:462, 50:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        merged_imgs[z, h*412:(h+1)*412, w*412:original_image_shape[2], 0] = imgs[z*H*W+ w*H+h][50:462, 50:original_image_shape[2]-412*W-50, 0]\n",
    "                    else:\n",
    "                        merged_imgs[z, h*412:(h+1)*412, w*412:(w+1)*412, 0] = imgs[z*H*W+ w*H+h][50:462, 50:462, 0]  \n",
    "                        \n",
    "    return merged_imgs\n",
    "\n",
    "\n",
    "def load_Y_gray_with_gaussian(folder_path, thresh = None , normalize = True, g_size = None):\n",
    "    image_files = []\n",
    "    #image_files = os.listdir(folder_path)\n",
    "    \n",
    "    for file in os.listdir(folder_path):\n",
    "        base, ext = os.path.splitext(file)\n",
    "        if ext == '.png':\n",
    "            image_files.append(file)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    image_files.sort()\n",
    "    print(image_files)\n",
    "    \n",
    "    img = cv2.imread(folder_path + os.sep + image_files[0], cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    images = np.zeros(\n",
    "        (len(image_files), img.shape[0], img.shape[1], 1) ,np.float32\n",
    "    )\n",
    "    \n",
    "    for i , image_file in enumerate(image_files):\n",
    "        image = cv2.imread(\n",
    "            folder_path + os.sep + image_file ,\n",
    "            cv2.IMREAD_GRAYSCALE\n",
    "        )\n",
    "        #print(image.shape)\n",
    "        \n",
    "        # ぼかし処理\n",
    "        if g_size:\n",
    "            image = cv2.GaussianBlur(\n",
    "                image, (g_size, g_size), 0\n",
    "            )\n",
    "        \n",
    "        if thresh:\n",
    "            ret , image = cv2.threshold(image , thresh , 255 , cv2.THRESH_BINARY)\n",
    "        image = image[ : , : , np.newaxis]\n",
    "        if normalize:\n",
    "            images[i] = normalize_y(image)\n",
    "        else:\n",
    "            images[i] = image\n",
    "            \n",
    "    print(images.shape)\n",
    "    \n",
    "    return images , image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cc511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_80_percents_data(imgs):\n",
    "    \n",
    "    \"\"\"\n",
    "        Args : \n",
    "            imgs (numpy.ndarray) : Z, Y, X, 1\n",
    "        \n",
    "        Returns :\n",
    "            imgs (numpy.ndarray) : Z, Y, X, 1\n",
    "            \n",
    "        Memo :\n",
    "            z, y, x　の全ての上下左右縦横10%を捨てる\n",
    "    \"\"\"\n",
    "    z_range = [int(imgs.shape[0] * 0.1), int(imgs.shape[0] * 0.9)]\n",
    "    y_range = [int(imgs.shape[1] * 0.1), int(imgs.shape[1] * 0.9)]\n",
    "    x_range = [int(imgs.shape[2] * 0.1), int(imgs.shape[2] * 0.9)]\n",
    "    \n",
    "    return imgs[z_range[0]:z_range[1], y_range[0]:y_range[1], x_range[0]:x_range[1], :]\n",
    "\n",
    "\n",
    "def make_cristae_train_datasets(ori_images, label_images, num_list):\n",
    "    \n",
    "    \"\"\"\n",
    "        Args : \n",
    "            ori_images (numpy.ndarray) : Z, Y, X, 1\n",
    "            label_images (numpy.ndarray) : Z, Y, X, 1\n",
    "            num_list (list) : num\n",
    "            \n",
    "        Returns : \n",
    "            train_ori_images (numpy.ndarray) : Z, Y, X, 1\n",
    "            train_label_images (numpy.ndarray) : Z, Y, X, 2\n",
    "            \n",
    "        Memo:\n",
    "            num_listに入っている画像を抽出\n",
    "            label_images は \"1:lamellar, 2:tubular\"\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    train_ori_images = list()\n",
    "    train_label_images = list()\n",
    "    for num in num_list:\n",
    "        ori_image = ori_images[num]\n",
    "        label_image = label_images[num]\n",
    "        \n",
    "        # 切り出す範囲を指定\n",
    "        x_range = [0, ori_image.shape[0]]\n",
    "        y_range = [0, ori_image.shape[1]]\n",
    "        \n",
    "        train_ori_images.append(ori_image[x_range[0]:x_range[1], y_range[0]:y_range[1], :])\n",
    "        train_label_images.append(label_image[x_range[0]:x_range[1], y_range[0]:y_range[1], :])\n",
    "        \n",
    "    train_label_images_001 = np.where(\n",
    "        np.array(train_label_images) == 1,\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    train_label_images_002 = np.where(\n",
    "        np.array(train_label_images) == 2,\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "        \n",
    "    cropped_ori_images = divide_gray_imgs(np.array(train_ori_images))\n",
    "    cropped_label_images_001 = divide_gray_imgs(np.array(train_label_images_001))\n",
    "    cropped_label_images_002 = divide_gray_imgs(np.array(train_label_images_002))\n",
    "    \n",
    "    cropped_ori_images = np.where(\n",
    "        cropped_ori_images == -1,\n",
    "        1,\n",
    "        cropped_ori_images\n",
    "    )\n",
    "    \n",
    "    cropped_label_images_001 = np.where(\n",
    "        cropped_label_images_001 == -1,\n",
    "        0,\n",
    "        cropped_label_images_001\n",
    "    )\n",
    "    \n",
    "    cropped_label_images_002 = np.where(\n",
    "        cropped_label_images_002 == -1,\n",
    "        0,\n",
    "        cropped_label_images_002\n",
    "    )\n",
    "    cropped_label_images = np.concatenate((cropped_label_images_001, cropped_label_images_002, np.zeros(cropped_label_images_001.shape)), axis = 3)\n",
    "        \n",
    "    return cropped_ori_images, cropped_label_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_white_mask_ori_img(ori_img, mask_img):\n",
    "    \n",
    "    masked_img = np.zeros(\n",
    "        (ori_img.shape[0], ori_img.shape[1], ori_img.shape[2], 1),\n",
    "        np.float32\n",
    "    )\n",
    "    print(masked_img.shape)\n",
    "    print(ori_img.shape)\n",
    "    print(mask_img.shape)\n",
    "    \n",
    "    for i in range(ori_img.shape[0]):\n",
    "        masked_img[:,:,:,0][i] = np.where(\n",
    "            mask_img[:,:,:,0][i] == 1,\n",
    "            ori_img[:,:,:,-1][i],\n",
    "            1\n",
    "        )\n",
    "        \n",
    "    return masked_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18996eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_no_mito_img(ori_images, label_images):\n",
    "    \"\"\"\n",
    "        Args : \n",
    "            ori_images (numpy.ndarray) : Z, Y, X, 1\n",
    "            label_images (numpy.ndarray) : Z, Y, X, 2\n",
    "            \n",
    "        Returns : \n",
    "            train_ori_images (numpy.ndarray) : Z, Y, X, 1\n",
    "            train_label_images (numpy.ndarray) : Z, Y, X, 2\n",
    "            \n",
    "        Memo:\n",
    "            cristaeのない画像を削除\n",
    "    \"\"\"\n",
    "    train_ori_images = list()\n",
    "    train_label_images = list()\n",
    "    \n",
    "    for ori, label in zip(ori_images, label_images):\n",
    "        if np.min(label) != np.max(label):\n",
    "            train_ori_images.append(ori)\n",
    "            train_label_images.append(label)\n",
    "    \n",
    "    return np.array(train_ori_images), np.array(train_label_images)\n",
    "\n",
    "\n",
    "\n",
    "def drop_no_label_mito(ori_images, mito_images, label_images):\n",
    "    \"\"\"\n",
    "        Args : \n",
    "            ori_images (numpy.ndarray) : Z, Y, X, 1\n",
    "            mito_images (numpy.ndarray) : Z, Y, X, 1\n",
    "            label_images (numpy.ndarray) : Z, Y, X, 3\n",
    "            \n",
    "        Returns : \n",
    "            train_ori_images (numpy.ndarray) : Z, Y, X, 1\n",
    "            train_label_images (numpy.ndarray) : Z, Y, X, 2\n",
    "            \n",
    "        Memo:\n",
    "            cristaeのないmitoを削除し、\n",
    "            cristaeが一枚もない画像を削除\n",
    "    \"\"\" \n",
    "\n",
    "    train_ori_images = list()\n",
    "    train_label_images = list()\n",
    "    \n",
    "    for ori, mito, label in zip(ori_images, mito_images, label_images):\n",
    "        label_imgs_, label_num = LABEL(mito)\n",
    "        \n",
    "        # label_numが１以上に制限\n",
    "        if label_num >= 1:\n",
    "            for num in range(label_num+1):\n",
    "                \n",
    "                # labelがついていないミトコン部位のoriを0にする\n",
    "                if (np.sum(np.where(label_imgs_==num, label, 0))) == 0:\n",
    "                    #print(ori.shape, label_imgs_.shape)\n",
    "                    ori = np.where(\n",
    "                        label_imgs_[:,:,0:1]==num,\n",
    "                        1,\n",
    "                        ori\n",
    "                    )\n",
    "                    #print(ori.shape, \"\\n\")\n",
    "        \n",
    "        # labelがない画像を削除\n",
    "        if np.min(label) != np.max(label):\n",
    "            train_ori_images.append(ori)\n",
    "            train_label_images.append(label)\n",
    "            \n",
    "        #print(ori.shape)\n",
    "        \n",
    "    return np.array(train_ori_images), np.array(train_label_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d3ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# クリステの間を塗る\n",
    "# Green（ラメラ）の間を塗る\n",
    "def make_gaps(label_imgs, mask_imgs):\n",
    "    \n",
    "    \"\"\"\n",
    "        Args :\n",
    "            label_imgs (numpy.ndarray) : Z, Y, X, 3\n",
    "            mask_imgs (numpy.ndarray) : Z, Y, X, 1\n",
    "            \n",
    "        Returns : \n",
    "            gap_imgs (numpy.ndarray) : Z, Y, X, 3\n",
    "    \"\"\"  \n",
    "    for i in range(label_imgs.shape[0]):\n",
    "        dilated_img = morphology.binary_dilation(label_imgs[i][:,:,1], morphology.disk(6)).astype(np.float32)\n",
    "        dilated_img = np.where(\n",
    "            label_imgs[i][:,:,0] == 0,\n",
    "            dilated_img,\n",
    "            0\n",
    "        )\n",
    "        dilated_img = np.where(\n",
    "            label_imgs[i][:,:,1] == 0,\n",
    "            dilated_img,\n",
    "            0\n",
    "        )\n",
    "        dilated_img = np.where(\n",
    "            mask_imgs[i][:,:,0] != 1,\n",
    "            dilated_img,\n",
    "            0\n",
    "        )\n",
    "     \n",
    "        label_imgs[i][:,:,2] = dilated_img\n",
    "        \n",
    "    label_imgs[:,:,:,0] = np.where(\n",
    "        label_imgs[:,:,:,1] > 0,\n",
    "        0,\n",
    "        label_imgs[:,:,:,0]\n",
    "    )\n",
    "        \n",
    "    return label_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57878353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Netのトレーニングを実行する関数\n",
    "def train_unet(X_train,Y_train, csv_path, model_path ,input_shape=(512, 512, 1), num_classes=1):\n",
    "    Y_train = Y_train\n",
    "    X_train = X_train\n",
    "    \n",
    "    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "    \n",
    "    # we create two instances with the same arguments\n",
    "    data_gen_args = dict(\n",
    "        rotation_range=90.,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        #shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True\n",
    "    )\n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
    "    seed = 1\n",
    "    image_datagen.fit(X_train, augment=True, seed=seed)\n",
    "    mask_datagen.fit(Y_train, augment=True, seed=seed)\n",
    "\n",
    "    image_generator = image_datagen.flow(X_train, seed=seed, batch_size=8)\n",
    "    mask_generator = mask_datagen.flow(Y_train, seed=seed, batch_size=8)\n",
    "\n",
    "    # combine generators into one which yields image and masks\n",
    "    train_generator = (pair for pair in zip(image_generator, mask_generator))\n",
    "    \n",
    "    #model = get_unet_512(input_shape=input_shape, num_classes=num_classes)\n",
    "    model = get_unet_resnet_512(input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "    BATCH_SIZE = 4\n",
    "    # 20エポック回せば十分\n",
    "    NUM_EPOCH = 400\n",
    "    \n",
    "    callbacks = []\n",
    "    from tensorflow.keras.callbacks import CSVLogger\n",
    "    callbacks.append(CSVLogger(csv_path))\n",
    "    history = model.fit_generator(train_generator,steps_per_epoch=32, epochs=NUM_EPOCH, verbose=1, callbacks=callbacks)\n",
    "    #history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, verbose=1, callbacks=callbacks)\n",
    "    model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670d5ef4",
   "metadata": {},
   "source": [
    "# shCtrl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1515466f",
   "metadata": {},
   "source": [
    "## cropped_001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da6d26d",
   "metadata": {},
   "source": [
    "### train_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_001/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_001/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521e01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_001_lst = [126, 186, 246]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e647a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ae7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba994f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_001/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_001.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb743e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bedcd80",
   "metadata": {},
   "source": [
    "### train_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1caf362",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_001/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb520a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_002//')\n",
    "\n",
    "num_001_lst = [126, 186, 246]\n",
    "num_002_lst = [194, 224, 237]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1117019",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f8c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_001/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_002.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f6e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d90db1",
   "metadata": {},
   "source": [
    "### train_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea671893",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_001/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e79933",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_003//')\n",
    "\n",
    "num_001_lst = [126, 186, 246]\n",
    "num_002_lst = [194, 224, 237]\n",
    "num_003_lst = [99, 156, 209]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda7c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462616c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_001/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_003.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_003_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add9042",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc510b",
   "metadata": {},
   "source": [
    "### train_004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b21064",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_001/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a84006",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_003//')\n",
    "cristae_004_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_004//')\n",
    "\n",
    "\n",
    "num_001_lst = [126, 186, 246]\n",
    "num_002_lst = [194, 224, 237]\n",
    "num_003_lst = [99, 156, 209]\n",
    "num_004_lst = [100, 157, 210]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n",
    "cropped_ori_004_imgs, cropped_label_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_004_imgs, num_004_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd34a798",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs, cropped_ori_004_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs, cropped_label_004_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eaa1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "_, cropped_mito_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_004_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs, cropped_mito_004_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d531cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_001/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_004.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_004_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb9dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc5a45",
   "metadata": {},
   "source": [
    "## cropped_002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6d3c1",
   "metadata": {},
   "source": [
    "### train_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee55100",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_002/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_002/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3ce894",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_002/manually_cristae_001/')\n",
    "\n",
    "num_001_lst = [52, 131, 174]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f54ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cffcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_002/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_001.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36b2259",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f1efb0",
   "metadata": {},
   "source": [
    "### train_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_002/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_002/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f111b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_002/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_002/manually_cristae_002//')\n",
    "\n",
    "num_001_lst = [52, 131, 174]\n",
    "num_002_lst = [35, 131, 208]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf9e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_002/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_002.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed4659",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3555aff",
   "metadata": {},
   "source": [
    "### train_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_002/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_002/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_002/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_002/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_002/manually_cristae_003//')\n",
    "\n",
    "num_001_lst = [52, 131, 174]\n",
    "num_002_lst = [35, 131, 208]\n",
    "num_003_lst = [79, 133, 200]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12942bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42794bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_002/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_003.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_003_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a769ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd53c9db",
   "metadata": {},
   "source": [
    "### train_004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_002/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_002/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3f01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_003//')\n",
    "cristae_004_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_001/manually_cristae_004//')\n",
    "\n",
    "\n",
    "num_001_lst = [52, 131, 174]\n",
    "num_002_lst = [35, 131, 208]\n",
    "num_003_lst = [79, 133, 200]\n",
    "num_004_lst = [100, 121, 180]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n",
    "cropped_ori_004_imgs, cropped_label_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_004_imgs, num_004_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d264925",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs, cropped_ori_004_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs, cropped_label_004_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a4f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "_, cropped_mito_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_004_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs, cropped_mito_004_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_002/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_004.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_004_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec3b13",
   "metadata": {},
   "source": [
    "## cropped_003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf0b80",
   "metadata": {},
   "source": [
    "### train_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf304ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_003/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9045eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/manually_cristae_001/')\n",
    "\n",
    "num_001_lst = [134, 213, 374]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af37f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa7312",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b0b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_003/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_001.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b17225",
   "metadata": {},
   "source": [
    "### train_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74eb81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_003/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d579654",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/manually_cristae_002//')\n",
    "\n",
    "num_001_lst = [134, 213, 374]\n",
    "num_002_lst = [124, 230, 309]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ba406",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7b91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_003/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_002.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01687e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd146aac",
   "metadata": {},
   "source": [
    "### train_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe0f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_003/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836df329",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/manually_cristae_003//')\n",
    "\n",
    "num_001_lst = [134, 213, 374]\n",
    "num_002_lst = [124, 230, 309]\n",
    "num_003_lst = [133, 135, 136]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd50709",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e739f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f3b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_003/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_003.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_003_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a39284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8067afe",
   "metadata": {},
   "source": [
    "## cropped_004"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5eaa5f",
   "metadata": {},
   "source": [
    "### train_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_004/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_004/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59201093",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_004/manually_cristae_001/')\n",
    "\n",
    "num_001_lst = [6, 211, 290]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa2f422",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_004/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_001.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b652a",
   "metadata": {},
   "source": [
    "### train_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e910dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_004/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_004/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d6936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_004/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_004/manually_cristae_002//')\n",
    "\n",
    "num_001_lst = [6, 211, 290]\n",
    "num_002_lst = [46, 94, 221]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa39391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab958c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dae9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_004/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_002.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d63de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c795cd9",
   "metadata": {},
   "source": [
    "### train_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_004/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_004/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/manually_cristae_003//')\n",
    "\n",
    "num_001_lst = [6, 211, 290]\n",
    "num_002_lst = [46, 94, 221]\n",
    "num_003_lst = [200]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481789e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_004/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_003.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_003_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1849ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7345f79",
   "metadata": {},
   "source": [
    "## cropped_005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb609b6",
   "metadata": {},
   "source": [
    "### train_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff4e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_005/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/manually_cristae_001/')\n",
    "\n",
    "num_001_lst = [20, 100, 180]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a14578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c325fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3668a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_005/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_001.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3242bf",
   "metadata": {},
   "source": [
    "### train_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea071b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_005/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ae542",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/manually_cristae_002//')\n",
    "\n",
    "num_001_lst = [20, 100, 180]\n",
    "num_002_lst = [60, 158, 237]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98979d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87070388",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_005/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_002.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad45f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e5211",
   "metadata": {},
   "source": [
    "### train_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9daa2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_005/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d898a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_003/manually_cristae_003//')\n",
    "\n",
    "num_001_lst = [20, 100, 180]\n",
    "num_002_lst = [60, 158, 237]\n",
    "num_003_lst = [91, 216, 236]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abfc037",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec79e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_005/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_003.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_003_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1bb1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed72842e",
   "metadata": {},
   "source": [
    "# shOPA1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0e8dd0",
   "metadata": {},
   "source": [
    "## cropped_001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ea30f",
   "metadata": {},
   "source": [
    "### train_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43efd2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_001/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_001/')\n",
    "\n",
    "num_001_lst = [53, 167, 351]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bafc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e0e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cda58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_001.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04254a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4df559",
   "metadata": {},
   "source": [
    "### train_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f8c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_001/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e643181",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_002//')\n",
    "\n",
    "num_001_lst = [53, 167, 351]\n",
    "num_002_lst = [76, 268, 313]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e285f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30195a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_002.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79053d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39cff46",
   "metadata": {},
   "source": [
    "### train_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f64e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_001/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_003//')\n",
    "\n",
    "num_001_lst = [53, 167, 351]\n",
    "num_002_lst = [76, 268, 313]\n",
    "num_003_lst = [114, 285, 336]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d588fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ef56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_003.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_003_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e49043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f949c977",
   "metadata": {},
   "source": [
    "### train_004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca17df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_001/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7961976",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_003//')\n",
    "cristae_004_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_004//')\n",
    "\n",
    "num_001_lst = [53, 167, 351]\n",
    "num_002_lst = [76, 268, 313]\n",
    "num_003_lst = [114, 285, 336]\n",
    "num_004_lst = [47, 62, 209]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n",
    "cropped_ori_004_imgs, cropped_label_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_004_imgs, num_004_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44df902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs, cropped_ori_004_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs, cropped_label_004_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba393d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "_, cropped_mito_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_004_lst)\n",
    "\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs, cropped_mito_004_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_004.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_004_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b6b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ca9a7",
   "metadata": {},
   "source": [
    "### train_005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41129603",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_001/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8537eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_003//')\n",
    "cristae_004_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_004//')\n",
    "cristae_005_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_005//')\n",
    "\n",
    "num_001_lst = [53, 167, 351]\n",
    "num_002_lst = [76, 268, 313]\n",
    "num_003_lst = [114, 285, 336]\n",
    "num_004_lst = [47, 62, 209]\n",
    "num_005_lst = [79, 220, 387]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n",
    "cropped_ori_004_imgs, cropped_label_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_004_imgs, num_004_lst)\n",
    "cropped_ori_005_imgs, cropped_label_005_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_005_imgs, num_005_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs, cropped_ori_004_imgs, cropped_ori_005_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs, cropped_label_004_imgs, cropped_label_005_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e074dfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "_, cropped_mito_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_004_lst)\n",
    "_, cropped_mito_005_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_005_lst)\n",
    "\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs, cropped_mito_004_imgs, cropped_mito_005_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_005.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_005_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13567fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1904ced5",
   "metadata": {},
   "source": [
    "### train_006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_001/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e839749",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_003//')\n",
    "cristae_004_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_004//')\n",
    "cristae_005_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_005//')\n",
    "cristae_006_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_006//')\n",
    "\n",
    "num_001_lst = [53, 167, 351]\n",
    "num_002_lst = [76, 268, 313]\n",
    "num_003_lst = [114, 285, 336]\n",
    "num_004_lst = [47, 62, 209]\n",
    "num_005_lst = [79, 220, 387]\n",
    "num_006_lst = [20, 293, 338]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n",
    "cropped_ori_004_imgs, cropped_label_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_004_imgs, num_004_lst)\n",
    "cropped_ori_005_imgs, cropped_label_005_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_005_imgs, num_005_lst)\n",
    "cropped_ori_006_imgs, cropped_label_006_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_006_imgs, num_006_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs, cropped_ori_004_imgs, cropped_ori_005_imgs, cropped_ori_006_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs, cropped_label_004_imgs, cropped_label_005_imgs, cropped_label_006_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae5e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "_, cropped_mito_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_004_lst)\n",
    "_, cropped_mito_005_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_005_lst)\n",
    "_, cropped_mito_006_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_006_lst)\n",
    "\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs, cropped_mito_004_imgs, cropped_mito_005_imgs, cropped_mito_006_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a92be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_006.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_006_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a3c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a5fb0",
   "metadata": {},
   "source": [
    "### train_007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc91d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_001/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_003//')\n",
    "cristae_004_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_004//')\n",
    "cristae_005_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_005//')\n",
    "cristae_006_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_006//')\n",
    "cristae_007_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_007//')\n",
    "\n",
    "num_001_lst = [53, 167, 351]\n",
    "num_002_lst = [76, 268, 313]\n",
    "num_003_lst = [114, 285, 336]\n",
    "num_004_lst = [47, 62, 209]\n",
    "num_005_lst = [79, 220, 387]\n",
    "num_006_lst = [20, 293, 338]\n",
    "num_007_lst = [48, 78, 368]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n",
    "cropped_ori_004_imgs, cropped_label_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_004_imgs, num_004_lst)\n",
    "cropped_ori_005_imgs, cropped_label_005_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_005_imgs, num_005_lst)\n",
    "cropped_ori_006_imgs, cropped_label_006_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_006_imgs, num_006_lst)\n",
    "cropped_ori_007_imgs, cropped_label_007_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_007_imgs, num_007_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c078037",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs, cropped_ori_004_imgs, cropped_ori_005_imgs, cropped_ori_006_imgs, cropped_ori_007_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs, cropped_label_004_imgs, cropped_label_005_imgs, cropped_label_006_imgs, cropped_label_007_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b23814",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "_, cropped_mito_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_004_lst)\n",
    "_, cropped_mito_005_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_005_lst)\n",
    "_, cropped_mito_006_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_006_lst)\n",
    "_, cropped_mito_007_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_007_lst)\n",
    "\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs, cropped_mito_004_imgs, cropped_mito_005_imgs, cropped_mito_006_imgs, cropped_mito_007_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475455ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_007.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_007_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9381c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5ad402",
   "metadata": {},
   "source": [
    "## cropped_002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533cee73",
   "metadata": {},
   "source": [
    "### train_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7276ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_002/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5afa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_001/')\n",
    "\n",
    "num_001_lst = [38, 157, 276]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76a91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ea715",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_002/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_001.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710a6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286025e",
   "metadata": {},
   "source": [
    "### train_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_002/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_002//')\n",
    "\n",
    "num_001_lst = [38, 157, 276]\n",
    "num_002_lst = [51, 97, 203]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a441440",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9244762",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a85dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_002/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_002.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6c87d",
   "metadata": {},
   "source": [
    "### train_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16063f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_002/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fee602",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_003//')\n",
    "\n",
    "num_001_lst = [38, 157, 276]\n",
    "num_002_lst = [51, 97, 203]\n",
    "num_003_lst = [54, 112, 183]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09faf7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_002/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_003.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_003_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d17573",
   "metadata": {},
   "source": [
    "### train_004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc901b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_002/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a964765",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_003//')\n",
    "cristae_004_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_004//')\n",
    "\n",
    "num_001_lst = [38, 157, 276]\n",
    "num_002_lst = [51, 97, 203]\n",
    "num_003_lst = [54, 112, 183]\n",
    "num_004_lst = [28, 73, 187]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n",
    "cropped_ori_004_imgs, cropped_label_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_004_imgs, num_004_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31586b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs, cropped_ori_004_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs, cropped_label_004_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f2a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "_, cropped_mito_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_004_lst)\n",
    "\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs, cropped_mito_004_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7fece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_002/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_004.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_004_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d56102",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a51923",
   "metadata": {},
   "source": [
    "### train_005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42cd048",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_002/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_003//')\n",
    "cristae_004_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_004//')\n",
    "cristae_005_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_005//')\n",
    "\n",
    "num_001_lst = [38, 157, 276]\n",
    "num_002_lst = [51, 97, 203]\n",
    "num_003_lst = [54, 112, 183]\n",
    "num_004_lst = [28, 73, 187]\n",
    "num_005_lst = [82, 181, 252]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n",
    "cropped_ori_004_imgs, cropped_label_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_004_imgs, num_004_lst)\n",
    "cropped_ori_005_imgs, cropped_label_005_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_005_imgs, num_005_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs, cropped_ori_004_imgs, cropped_ori_005_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs, cropped_label_004_imgs, cropped_label_005_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39ca303",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "_, cropped_mito_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_004_lst)\n",
    "_, cropped_mito_005_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_005_lst)\n",
    "\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs, cropped_mito_004_imgs, cropped_mito_005_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab0b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_002/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_005.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_005_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dcac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08d968",
   "metadata": {},
   "source": [
    "### train_006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ecf386",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_002/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c61ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_003//')\n",
    "cristae_004_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_004//')\n",
    "cristae_005_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_005//')\n",
    "cristae_006_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/manually_cristae_006//')\n",
    "\n",
    "num_001_lst = [38, 157, 276]\n",
    "num_002_lst = [51, 97, 203]\n",
    "num_003_lst = [54, 112, 183]\n",
    "num_004_lst = [28, 73, 187]\n",
    "num_005_lst = [82, 181, 252]\n",
    "num_006_lst = [69, 76, 197]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n",
    "cropped_ori_004_imgs, cropped_label_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_004_imgs, num_004_lst)\n",
    "cropped_ori_005_imgs, cropped_label_005_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_005_imgs, num_005_lst)\n",
    "cropped_ori_006_imgs, cropped_label_006_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_006_imgs, num_006_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs, cropped_ori_004_imgs, cropped_ori_005_imgs, cropped_ori_006_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs, cropped_label_004_imgs, cropped_label_005_imgs, cropped_label_006_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "_, cropped_mito_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_004_lst)\n",
    "_, cropped_mito_005_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_005_lst)\n",
    "_, cropped_mito_006_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_006_lst)\n",
    "\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs, cropped_mito_004_imgs, cropped_mito_005_imgs, cropped_mito_006_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c964492",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_002/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_006.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_006_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9967d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d975194a",
   "metadata": {},
   "source": [
    "### train_007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_002/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_002/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_003//')\n",
    "cristae_004_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_004//')\n",
    "cristae_005_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_005//')\n",
    "cristae_006_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_006//')\n",
    "cristae_007_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_001/manually_cristae_007//')\n",
    "\n",
    "num_001_lst = [38, 157, 276]\n",
    "num_002_lst = [51, 97, 203]\n",
    "num_003_lst = [54, 112, 183]\n",
    "num_004_lst = [28, 73, 187]\n",
    "num_005_lst = [82, 181, 252]\n",
    "num_006_lst = [69, 76, 197]\n",
    "num_007_lst = [72, 78, 81]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n",
    "cropped_ori_004_imgs, cropped_label_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_004_imgs, num_004_lst)\n",
    "cropped_ori_005_imgs, cropped_label_005_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_005_imgs, num_005_lst)\n",
    "cropped_ori_006_imgs, cropped_label_006_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_006_imgs, num_006_lst)\n",
    "cropped_ori_007_imgs, cropped_label_007_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_007_imgs, num_007_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4940e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs, cropped_ori_003_imgs, cropped_ori_004_imgs, cropped_ori_005_imgs, cropped_ori_006_imgs, cropped_ori_007_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs, cropped_label_004_imgs, cropped_label_005_imgs, cropped_label_006_imgs, cropped_label_007_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29164c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "_, cropped_mito_004_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_004_lst)\n",
    "_, cropped_mito_005_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_005_lst)\n",
    "_, cropped_mito_006_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_006_lst)\n",
    "_, cropped_mito_007_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_007_lst)\n",
    "\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs, cropped_mito_003_imgs, cropped_mito_004_imgs, cropped_mito_005_imgs, cropped_mito_006_imgs, cropped_mito_007_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36576391",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_002/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_007.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_007_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b9a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3c752a",
   "metadata": {},
   "source": [
    "## cropped_003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d39ede",
   "metadata": {},
   "source": [
    "### train_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e2082",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_003/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/manually_cristae_001/')\n",
    "\n",
    "num_001_lst = [78, 137, 230]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccb2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b396bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3c0dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_003/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_001.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b866a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87fa383",
   "metadata": {},
   "source": [
    "### train_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65500277",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_003/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2730374",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/manually_cristae_002//')\n",
    "\n",
    "num_001_lst = [78, 137, 230]\n",
    "num_002_lst = [9, 246, 354]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ee5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_003/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_002.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13fb622",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e4b223",
   "metadata": {},
   "source": [
    "## cropped_004"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49567f98",
   "metadata": {},
   "source": [
    "### train_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a61172",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_004/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_004/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83cb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_004/manually_cristae_001/')\n",
    "\n",
    "num_001_lst = [111, 188, 284, 314]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ef702",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_004/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_001.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850895b",
   "metadata": {},
   "source": [
    "### train_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7268eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_004/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_004/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfa730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_004/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_004/manually_cristae_002//')\n",
    "\n",
    "num_001_lst = [111, 188, 284, 314]\n",
    "num_002_lst = [183, 193, 309 ]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b781d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed7793",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_004/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_002.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1873e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4094e3",
   "metadata": {},
   "source": [
    "## cropped_005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c0342d",
   "metadata": {},
   "source": [
    "### train_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ddbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_005/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea85a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/manually_cristae_001/')\n",
    "\n",
    "num_001_lst = [150, 250, 350, 450]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs], axis=0)\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb3ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b0b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_005/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_001.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c45a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216bf4af",
   "metadata": {},
   "source": [
    "### train_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5273a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_005/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/manually_cristae_002//')\n",
    "\n",
    "num_001_lst = [150, 250, 350, 450]\n",
    "num_002_lst = [45, 93, 144, 236]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedff109",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs, cropped_ori_002_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05682b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_005/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_002.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab181e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c65aa82",
   "metadata": {},
   "source": [
    "### train_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b67bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs , _ = load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_005/resize_10x10x10')\n",
    "mito_imgs , _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/devided_mito//')\n",
    "\n",
    "masked_imgs = make_white_mask_ori_img(ori_imgs, mito_imgs)\n",
    "cropped_masked_imgs = crop_80_percents_data(masked_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cristae_001_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/manually_cristae_001//')\n",
    "cristae_002_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/manually_cristae_002//')\n",
    "cristae_003_imgs, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/manually_cristae_003//')\n",
    "\n",
    "num_001_lst = [150, 250, 350, 450]\n",
    "num_002_lst = [45, 93, 144, 236]\n",
    "num_003_lst = [16, 240, 311]\n",
    "\n",
    "cropped_ori_001_imgs, cropped_label_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_001_imgs, num_001_lst)\n",
    "cropped_ori_002_imgs, cropped_label_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_002_imgs, num_002_lst)\n",
    "cropped_ori_003_imgs, cropped_label_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cristae_003_imgs, num_003_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_ori_imgs = np.concatenate([cropped_ori_001_imgs,  cropped_ori_002_imgs, cropped_ori_003_imgs], axis=0)\n",
    "cropped_label_imgs = np.concatenate([cropped_label_001_imgs, cropped_label_002_imgs, cropped_label_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs, train_label_imgs = drop_no_mito_img(cropped_ori_imgs, cropped_label_imgs)\n",
    "train_gap_label_imgs = make_gaps(train_label_imgs, train_ori_imgs)\n",
    "train_gap_label_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_mito_imgs = crop_80_percents_data(mito_imgs)\n",
    "_, cropped_mito_001_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_001_lst)\n",
    "_, cropped_mito_002_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_002_lst)\n",
    "_, cropped_mito_003_imgs = make_cristae_train_datasets(cropped_masked_imgs, cropped_mito_imgs, num_003_lst)\n",
    "\n",
    "cropped_mito_imgs = np.concatenate([cropped_mito_001_imgs, cropped_mito_002_imgs,  cropped_mito_003_imgs], axis=0)\n",
    "\n",
    "train_ori_imgs_, train_label_imgs_ = drop_no_label_mito(cropped_ori_imgs, cropped_mito_imgs, cropped_label_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd5cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_005/cristae_active_learning\"\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "csv_path = f'{out_dir}/cristae_unet_resblock_003.csv'\n",
    "X_train = train_ori_imgs_\n",
    "Y_train = train_gap_label_imgs\n",
    "model_path = f'{out_dir}/cristae_unet_resblock_003_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b7291",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unet(X_train, Y_train, csv_path, model_path, input_shape, num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
