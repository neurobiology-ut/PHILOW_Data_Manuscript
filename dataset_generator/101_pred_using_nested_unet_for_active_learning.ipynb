{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bda930",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]= \"true\"\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import Adam , RMSprop\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint , EarlyStopping , CSVLogger\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input , concatenate , Conv2D , MaxPooling2D , Activation , UpSampling2D , BatchNormalization, Conv2DTranspose\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57788655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#値を-1から1に正規化する関数\n",
    "def normalize_x(image):\n",
    "    return image / 127.5 - 1\n",
    "\n",
    "\n",
    "def denormalize_x(image):\n",
    "    return (image + 1) * 127.5\n",
    "\n",
    "\n",
    "#値を0から1正規化する関数\n",
    "def normalize_y(image):\n",
    "    return image / 255\n",
    "\n",
    "\n",
    "#値を0から255に戻す関数\n",
    "def denormalize_y(image):\n",
    "    return image * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecdece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インプット画像を読み込む関数\n",
    "def load_X_gray(folder_path):\n",
    "    \n",
    "    image_files = []\n",
    "\n",
    "    #image_files = os.listdir(folder_path)\n",
    "       \n",
    "    for file in os.listdir(folder_path):\n",
    "        base, ext = os.path.splitext(file)\n",
    "        if ext == '.png':\n",
    "            image_files.append(file)\n",
    "        else :\n",
    "            pass\n",
    "        \n",
    "    image_files.sort()\n",
    "    \n",
    "    img = cv2.imread(folder_path + os.sep + image_files[0], cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    #image_files = image_files[1:]\n",
    "    images = np.zeros((len(image_files), img.shape[0], img.shape[1], 1), np.float32)\n",
    "    for i, image_file in tqdm(enumerate(image_files)):\n",
    "        image = cv2.imread(folder_path + os.sep + image_file, cv2.IMREAD_GRAYSCALE)\n",
    "        #image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        image = image[:, :, np.newaxis]\n",
    "        images[i] = normalize_x(image)\n",
    "    \n",
    "    print(images.shape)\n",
    "    \n",
    "    return images, image_files\n",
    "\n",
    "\n",
    "def load_Y_gray(folder_path, thresh = None , normalize = True, g_size = None):\n",
    "    image_files = []\n",
    "    #image_files = os.listdir(folder_path)\n",
    "    \n",
    "    for file in os.listdir(folder_path):\n",
    "        base, ext = os.path.splitext(file)\n",
    "        if ext == '.png':\n",
    "            image_files.append(file)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    image_files.sort()\n",
    "    \n",
    "    img = cv2.imread(folder_path + os.sep + image_files[0], cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    images = np.zeros(\n",
    "        (len(image_files), img.shape[0], img.shape[1], 1) ,np.float32\n",
    "    )\n",
    "    \n",
    "    for i , image_file in tqdm(enumerate(image_files)):\n",
    "        image = cv2.imread(\n",
    "            folder_path + os.sep + image_file ,\n",
    "            cv2.IMREAD_GRAYSCALE\n",
    "        )\n",
    "        #print(image.shape)\n",
    "        \n",
    "        # ぼかし処理\n",
    "        if g_size:\n",
    "            image = cv2.GaussianBlur(\n",
    "                image, (g_size, g_size), 0\n",
    "            )        \n",
    "        \n",
    "        if thresh:\n",
    "            ret , image = cv2.threshold(image , thresh , 255 , cv2.THRESH_BINARY)\n",
    "        image = image[ : , : , np.newaxis]\n",
    "        if normalize:\n",
    "            images[i] = normalize_y(image)\n",
    "        else:\n",
    "            images[i] = image\n",
    "            \n",
    "    print(images.shape)\n",
    "    \n",
    "    return images , image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_imgs(images):\n",
    "    \n",
    "    '''\n",
    "        512 x 512に分割する\n",
    "        端の50pxをのりしろとする\n",
    "        空欄箇所は-1（黒）にする\n",
    "        height, widthが412未満の場合は変数を-1に変更\n",
    "    '''\n",
    "    \n",
    "    H = -(-images.shape[1]//412)\n",
    "    W = -(-images.shape[2]//412)\n",
    "    \n",
    "    diveded_imgs = np.zeros(( images.shape[0]*H*W, 512, 512, 1), np.float32)\n",
    "    print(H,W)\n",
    "    \n",
    "    for z in range(images.shape[0]):\n",
    "        image = images[z]\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "                cropped_img = np.zeros((512, 512, 1), np.float32)\n",
    "                cropped_img -= 1\n",
    "                \n",
    "                if images.shape[1] < 412:\n",
    "                    h = -1\n",
    "                if images.shape[2] < 412:\n",
    "                    w = -1\n",
    "                    \n",
    "                if h == -1:\n",
    "                    if w == -1:\n",
    "                        cropped_img[50:images.shape[1]+50, 50:images.shape[2]+50, 0] = image[0:images.shape[1], 0:images.shape[2], 0]\n",
    "                    elif w == 0:\n",
    "                        cropped_img[50:images.shape[1]+50, 50:512, 0] = image[0:images.shape[1], 0:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        cropped_img[50:images.shape[1]+50, 0:images.shape[2]-412*W-50, 0] = image[0:images.shape[1], w*412-50:images.shape[2], 0]\n",
    "                    else:\n",
    "                        cropped_img[50:images.shape[1]+50, :, 0] = image[0:images.shape[1], w*412-50:(w+1)*412+50, 0]\n",
    "                elif h == 0:\n",
    "                    if w == -1:\n",
    "                        cropped_img[50:512, 50:images.shape[2]+50, 0] = image[0:462, 0:images.shape[2], 0]\n",
    "                    elif w == 0:\n",
    "                        cropped_img[50:512, 50:512, 0] = image[0:462, 0:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        cropped_img[50:512, 0:images.shape[2]-412*W-50, 0] = image[0:462, w*412-50:images.shape[2], 0]\n",
    "                    else:\n",
    "                        #cropped_img[50:512, :, 0] = image[0:462, w*412-50:(w+1)*412+50, 0]\n",
    "                        try:\n",
    "                            cropped_img[50:512, :, 0] = image[0:462, w*412-50:(w+1)*412+50, 0]\n",
    "                        except:\n",
    "                            cropped_img[50:512, 0:images.shape[2]-412*(W-1)-50, 0] = image[0:462, w*412-50:(w+1)*412+50, 0]\n",
    "                elif h == H-1:\n",
    "                    if w == -1:\n",
    "                        cropped_img[0:images.shape[1]-412*H-50, 50:images.shape[2]+50, 0] = image[h*412-50:images.shape[1], 0:images.shape[2], 0]\n",
    "                    elif w == 0:\n",
    "                        cropped_img[0:images.shape[1]-412*H-50, 50:512, 0] = image[h*412-50:images.shape[1], 0:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        cropped_img[0:images.shape[1]-412*H-50, 0:images.shape[2]-412*W-50, 0] = image[h*412-50:images.shape[1], w*412-50:images.shape[2], 0]\n",
    "                    else:\n",
    "                        try:\n",
    "                            cropped_img[0:images.shape[1]-412*H-50, :, 0] = image[h*412-50:images.shape[1], w*412-50:(w+1)*412+50, 0]\n",
    "                        except:\n",
    "                            cropped_img[0:images.shape[1]-412*H-50, 0:images.shape[2]-412*(W-1)-50, 0] = image[h*412-50:images.shape[1], w*412-50:(w+1)*412+50, 0]\n",
    "                else:\n",
    "                    if w == -1:\n",
    "                        cropped_img[:, 50:images.shape[2]+50, 0] = image[h*412-50:(h+1)*412+50, 0:images.shape[2], 0]\n",
    "                    elif w == 0:\n",
    "                        #cropped_img[:, 50:512, 0] = image[h*412-50:(h+1)*412+50, 0:462, 0]\n",
    "                        try:\n",
    "                            cropped_img[:, 50:512, 0] = image[h*412-50:(h+1)*412+50, 0:462, 0]\n",
    "                        except:\n",
    "                            cropped_img[0:images.shape[1]-412*H-50+412, 50:512, 0] = image[h*412-50:(h+1)*412+50, 0:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        #cropped_img[:, 0:images.shape[2]-412*W-50, 0] = image[h*412-50:(h+1)*412+50, w*412-50:images.shape[2], 0]\n",
    "                        try:\n",
    "                            cropped_img[:, 0:images.shape[2]-412*W-50, 0] = image[h*412-50:(h+1)*412+50, w*412-50:images.shape[2], 0]\n",
    "                        except:\n",
    "                            cropped_img[0:images.shape[1]-412*H-50+412, 0:images.shape[2]-412*W-50, 0] = image[h*412-50:(h+1)*412+50, w*412-50:images.shape[2], 0]\n",
    "                    else:\n",
    "                        #cropped_img[:, :, 0] = image[h*412-50:(h+1)*412+50, w*412-50:(w+1)*412+50, 0]\n",
    "                        try:\n",
    "                            cropped_img[:, :, 0] = image[h*412-50:(h+1)*412+50, w*412-50:(w+1)*412+50, 0]         \n",
    "                        except:\n",
    "                            try:\n",
    "                                 cropped_img[:, 0:images.shape[2]-412*(W-1)-50, 0] = image[h*412-50:(h+1)*412+50, w*412-50:(w+1)*412+50, 0]\n",
    "                            except:\n",
    "                                 cropped_img[0:images.shape[1]-412*(H-1)-50, :, 0] = image[h*412-50:(h+1)*412+50, w*412-50:(w+1)*412+50, 0]\n",
    "                h = max(0, h)\n",
    "                w = max(0, w)\n",
    "                diveded_imgs[z*H*W+ w*H+h] = cropped_img\n",
    "                #print(z*H*W+ w*H+h)\n",
    "                \n",
    "    return diveded_imgs\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def merge_imgs(imgs, original_image_shape):\n",
    "    \n",
    "    '''\n",
    "        元のサイズに合体させる\n",
    "        original_image_shapeはoriginal_image.shapeで与えられる、(z, h, w, channel)のタプル型\n",
    "    '''\n",
    "    \n",
    "    merged_imgs = np.zeros((original_image_shape[0], original_image_shape[1], original_image_shape[2], 1), np.float32)\n",
    "    H = -(-original_image_shape[1]//412)\n",
    "    W = -(-original_image_shape[2]//412)    \n",
    "    \n",
    "    for z in range(original_image_shape[0]):\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "\n",
    "                if original_image_shape[1] < 412:\n",
    "                    h = -1\n",
    "                if original_image_shape[2] < 412:\n",
    "                    w = -1\n",
    "                    \n",
    "                #print(z*H*W+ max(w, 0)*H+max(h, 0))    \n",
    "                if h == -1:\n",
    "                    if w == -1:\n",
    "                        merged_imgs[z, 0:original_image_shape[1], 0:original_image_shape[2], 0] = imgs[z*H*W+ 0*H+0][50:original_image_shape[1]+50, 50:original_image_shape[2]+50, 0]\n",
    "                    elif w == 0:\n",
    "                        merged_imgs[z, 0:original_image_shape[1], 0:412, 0] = imgs[z*H*W+ w*H+0][50:original_image_shape[1]+50, 50:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        merged_imgs[z, 0:original_image_shape[1], w*412:original_image_shape[2], 0] = imgs[z*H*W+ w*H+0][50:original_image_shape[1]+50, 50:original_image_shape[2]-412*W-50, 0]\n",
    "                    else:\n",
    "                        merged_imgs[z, 0:original_image_shape[1], w*412:(w+1)*412, 0] = imgs[z*H*W+ w*H+0][50:original_image_shape[1]+50, 50:462, 0]\n",
    "                elif h == 0:\n",
    "                    if w == -1:\n",
    "                        merged_imgs[z, 0:412, 0:original_image_shape[2], 0] = imgs[z*H*W+ 0*H+h][50:462, 50:original_image_shape[2]+50, 0]\n",
    "                    elif w == 0:\n",
    "                        merged_imgs[z, 0:412, 0:412, 0] = imgs[z*H*W+ w*H+h][50:462, 50:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        merged_imgs[z, 0:412, w*412:original_image_shape[2], 0] = imgs[z*H*W+ w*H+h][50:462, 50:original_image_shape[2]-412*W-50, 0]\n",
    "                    else:\n",
    "                        merged_imgs[z, 0:412, w*412:(w+1)*412, 0] = imgs[z*H*W+ w*H+h][50:462, 50:462, 0]\n",
    "                elif h == H-1:\n",
    "                    if w == -1:\n",
    "                         merged_imgs[z, h*412:original_image_shape[1], 0:original_image_shape[2], 0] = imgs[z*H*W+ 0*H+h][50:original_image_shape[1]-412*H-50, 50:original_image_shape[2]+50, 0]\n",
    "                    elif w == 0:\n",
    "                        merged_imgs[z, h*412:original_image_shape[1], 0:412, 0] = imgs[z*H*W+ w*H+h][50:original_image_shape[1]-412*H-50, 50:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        merged_imgs[z, h*412:original_image_shape[1], w*412:original_image_shape[2], 0] = imgs[z*H*W+ w*H+h][50:original_image_shape[1]-412*H-50, 50:original_image_shape[2]-412*W-50, 0]\n",
    "                    else:\n",
    "                        merged_imgs[z, h*412:original_image_shape[1], w*412:(w+1)*412, 0] = imgs[z*H*W+ w*H+h][50:original_image_shape[1]-412*H-50, 50:462, 0]\n",
    "                else:\n",
    "                    if w == -1:\n",
    "                         merged_imgs[z, h*412:(h+1)*412, 0:original_image_shape[2], 0] = imgs[z*H*W+ 0*H+h][50:462, 50:original_image_shape[2]+50, 0]\n",
    "                    elif w == 0:\n",
    "                        merged_imgs[z, h*412:(h+1)*412, 0:412, 0] = imgs[z*H*W+ w*H+h][50:462, 50:462, 0]\n",
    "                    elif w == W-1:\n",
    "                        merged_imgs[z, h*412:(h+1)*412, w*412:original_image_shape[2], 0] = imgs[z*H*W+ w*H+h][50:462, 50:original_image_shape[2]-412*W-50, 0]\n",
    "                    else:\n",
    "                        merged_imgs[z, h*412:(h+1)*412, w*412:(w+1)*412, 0] = imgs[z*H*W+ w*H+h][50:462, 50:462, 0]  \n",
    "        \n",
    "    print(merged_imgs.shape)\n",
    "    return merged_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03cc13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_uint(input_tensor, nb_filter):\n",
    "    \n",
    "    x = Conv2D(nb_filter, (3, 3), padding='same')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(nb_filter, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def get_nest_unet_512_layer_7(input_shape=(512, 512, 1), num_classes=3, deep_supervision = False):\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        \n",
    "        inputs = Input(shape = input_shape)\n",
    "        \n",
    "        # 512\n",
    "        conv1_1 = standard_uint(inputs, nb_filter = 16)\n",
    "        pool1 = MaxPooling2D((2, 2), strides = (2, 2))(conv1_1)\n",
    "        \n",
    "        #256\n",
    "        conv2_1 = standard_uint(pool1, nb_filter = 32)\n",
    "        pool2 = MaxPooling2D((2, 2), strides = (2, 2))(conv2_1)\n",
    "        \n",
    "        up1_2 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_1)\n",
    "        conv1_2 = concatenate([up1_2, conv1_1], axis = 3)\n",
    "        conv1_2 = standard_uint(conv1_2, nb_filter = 16)\n",
    "        \n",
    "        #128\n",
    "        conv3_1 = standard_uint(pool2, nb_filter = 64)\n",
    "        pool3 = MaxPooling2D((2, 2), strides = (2, 2))(conv3_1)\n",
    "        \n",
    "        up2_2 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_1)\n",
    "        conv2_2 = concatenate([up2_2, conv2_1], axis = 3)\n",
    "        conv2_2 = standard_uint(conv2_2, nb_filter = 32)\n",
    "        \n",
    "        up1_3 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_2)\n",
    "        conv1_3 = concatenate([up1_3, conv1_1, conv1_2], axis = 3)\n",
    "        conv1_3 = standard_uint(conv1_3, nb_filter = 16)\n",
    "        \n",
    "        # 64\n",
    "        conv4_1 = standard_uint(pool3, nb_filter = 128)\n",
    "        pool4 = MaxPooling2D((2, 2), strides = (2, 2))(conv4_1)      \n",
    "        \n",
    "        up3_2 = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = 'same')(conv4_1)\n",
    "        conv3_2 = concatenate([up3_2, conv3_1], axis = 3)\n",
    "        conv3_2 = standard_uint(conv3_2, nb_filter = 64)    \n",
    "        \n",
    "        up2_3 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_2)\n",
    "        conv2_3 = concatenate([up2_3, conv2_1, conv2_2], axis = 3)\n",
    "        conv2_3 = standard_uint(conv2_3, nb_filter = 32)        \n",
    "        \n",
    "        up1_4 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_3)\n",
    "        conv1_4 = concatenate([up1_4, conv1_2, conv1_3], axis = 3)\n",
    "        conv1_4 = standard_uint(conv1_4, nb_filter = 16)       \n",
    "        \n",
    "        # 32\n",
    "        conv5_1 = standard_uint(pool4, nb_filter = 256)\n",
    "        pool5 = MaxPooling2D((2, 2), strides = (2, 2))(conv5_1)      \n",
    "        \n",
    "        up4_2 = Conv2DTranspose(128, (2, 2), strides = (2, 2), padding = 'same')(conv5_1)\n",
    "        conv4_2 = concatenate([up4_2, conv4_1], axis = 3)\n",
    "        conv4_2 = standard_uint(conv4_2, nb_filter = 128)  \n",
    "        \n",
    "        up3_3 = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = 'same')(conv4_2)\n",
    "        conv3_3 = concatenate([up3_3, conv3_1, conv3_2], axis = 3)\n",
    "        conv3_3 = standard_uint(conv3_3, nb_filter = 64)\n",
    "        \n",
    "        up2_4 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_3)\n",
    "        conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], axis = 3)\n",
    "        conv2_4 = standard_uint(conv2_4, nb_filter = 32)\n",
    "        \n",
    "        up1_5 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_4)\n",
    "        conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], axis = 3)\n",
    "        conv1_5 = standard_uint(conv1_5, nb_filter = 16)\n",
    "        \n",
    "        \n",
    "        # 16\n",
    "        conv6_1 = standard_uint(pool5, nb_filter = 512)        \n",
    "        \n",
    "        up5_2 = Conv2DTranspose(256, (2, 2), strides = (2, 2), padding = 'same')(conv6_1)\n",
    "        conv5_2 = concatenate([up5_2, conv5_1], axis = 3)\n",
    "        conv5_2 = standard_uint(conv5_2, nb_filter = 256) \n",
    "        \n",
    "        up4_3 = Conv2DTranspose(128, (2, 2), strides = (2, 2), padding = 'same')(conv5_2)\n",
    "        conv4_3 = concatenate([up4_3, conv4_1, conv4_2], axis = 3)\n",
    "        conv4_3 = standard_uint(conv4_3, nb_filter = 128)\n",
    "        \n",
    "        up3_4 = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = 'same')(conv4_3)\n",
    "        conv3_4 = concatenate([up3_4, conv3_1, conv3_2, conv3_3], axis = 3)\n",
    "        conv3_4 = standard_uint(conv3_4, nb_filter = 64)\n",
    "        \n",
    "        up2_5 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_4)\n",
    "        conv2_5 = concatenate([up2_5, conv2_1, conv2_2, conv2_3, conv2_4], axis = 3)\n",
    "        conv2_5 = standard_uint(conv2_5, nb_filter = 32)   \n",
    "        \n",
    "        up1_6 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_5)\n",
    "        conv1_6 = concatenate([up1_6, conv1_1, conv1_2, conv1_3, conv1_4, conv1_5], axis = 3)\n",
    "        conv1_6 = standard_uint(conv1_6, nb_filter = 16)   \n",
    "        \n",
    "        \n",
    "        nested_output_1 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_2)\n",
    "        nested_output_2 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_3)\n",
    "        nested_output_3 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_4)\n",
    "        nested_output_4 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_5)\n",
    "        nested_output_5 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_6)\n",
    "        \n",
    "        if deep_supervision:\n",
    "            model = Model(inputs = inputs, outputs = [nested_output_1, nested_output_2, nested_output_3, nested_output_4, nested_output_5]) \n",
    "        else:\n",
    "            model = Model(inputs = inputs, outputs = [nested_output_5])\n",
    "            \n",
    "        model.compile(optimizer=RMSprop(lr=0.0001), loss=bce_dice_loss, metrics=[dice_coeff])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "   \n",
    "\n",
    "def get_nest_unet_512_layer_6(input_shape=(512, 512, 1), num_classes=3, deep_supervision = False):\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        \n",
    "        inputs = Input(shape = input_shape)\n",
    "        \n",
    "        # 512\n",
    "        conv1_1 = standard_uint(inputs, nb_filter = 16)\n",
    "        pool1 = MaxPooling2D((2, 2), strides = (2, 2))(conv1_1)\n",
    "        \n",
    "        #256\n",
    "        conv2_1 = standard_uint(pool1, nb_filter = 32)\n",
    "        pool2 = MaxPooling2D((2, 2), strides = (2, 2))(conv2_1)\n",
    "        \n",
    "        up1_2 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_1)\n",
    "        conv1_2 = concatenate([up1_2, conv1_1], axis = 3)\n",
    "        conv1_2 = standard_uint(conv1_2, nb_filter = 16)\n",
    "        \n",
    "        #128\n",
    "        conv3_1 = standard_uint(pool2, nb_filter = 64)\n",
    "        pool3 = MaxPooling2D((2, 2), strides = (2, 2))(conv3_1)\n",
    "        \n",
    "        up2_2 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_1)\n",
    "        conv2_2 = concatenate([up2_2, conv2_1], axis = 3)\n",
    "        conv2_2 = standard_uint(conv2_2, nb_filter = 32)\n",
    "        \n",
    "        up1_3 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_2)\n",
    "        conv1_3 = concatenate([up1_3, conv1_1, conv1_2], axis = 3)\n",
    "        conv1_3 = standard_uint(conv1_3, nb_filter = 16)\n",
    "        \n",
    "        # 64\n",
    "        conv4_1 = standard_uint(pool3, nb_filter = 128)\n",
    "        pool4 = MaxPooling2D((2, 2), strides = (2, 2))(conv4_1)      \n",
    "        \n",
    "        up3_2 = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = 'same')(conv4_1)\n",
    "        conv3_2 = concatenate([up3_2, conv3_1], axis = 3)\n",
    "        conv3_2 = standard_uint(conv3_2, nb_filter = 64)    \n",
    "        \n",
    "        up2_3 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_2)\n",
    "        conv2_3 = concatenate([up2_3, conv2_1, conv2_2], axis = 3)\n",
    "        conv2_3 = standard_uint(conv2_3, nb_filter = 32)        \n",
    "        \n",
    "        up1_4 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_3)\n",
    "        conv1_4 = concatenate([up1_4, conv1_2, conv1_3], axis = 3)\n",
    "        conv1_4 = standard_uint(conv1_4, nb_filter = 16)       \n",
    "        \n",
    "        # 32\n",
    "        conv5_1 = standard_uint(pool4, nb_filter = 256)\n",
    "        pool5 = MaxPooling2D((2, 2), strides = (2, 2))(conv5_1)      \n",
    "        \n",
    "        up4_2 = Conv2DTranspose(128, (2, 2), strides = (2, 2), padding = 'same')(conv5_1)\n",
    "        conv4_2 = concatenate([up4_2, conv4_1], axis = 3)\n",
    "        conv4_2 = standard_uint(conv4_2, nb_filter = 128)  \n",
    "        \n",
    "        up3_3 = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = 'same')(conv4_2)\n",
    "        conv3_3 = concatenate([up3_3, conv3_1, conv3_2], axis = 3)\n",
    "        conv3_3 = standard_uint(conv3_3, nb_filter = 64)\n",
    "        \n",
    "        up2_4 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_3)\n",
    "        conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], axis = 3)\n",
    "        conv2_4 = standard_uint(conv2_4, nb_filter = 32)\n",
    "        \n",
    "        up1_5 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_4)\n",
    "        conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], axis = 3)\n",
    "        conv1_5 = standard_uint(conv1_5, nb_filter = 16)\n",
    "        \n",
    "        \n",
    "        # 16\n",
    "        conv6_1 = standard_uint(pool5, nb_filter = 512)\n",
    "        pool6 = MaxPooling2D((2, 2), strides = (2, 2))(conv6_1)        \n",
    "        \n",
    "        up5_2 = Conv2DTranspose(256, (2, 2), strides = (2, 2), padding = 'same')(conv6_1)\n",
    "        conv5_2 = concatenate([up5_2, conv5_1], axis = 3)\n",
    "        conv5_2 = standard_uint(conv5_2, nb_filter = 256) \n",
    "        \n",
    "        up4_3 = Conv2DTranspose(128, (2, 2), strides = (2, 2), padding = 'same')(conv5_2)\n",
    "        conv4_3 = concatenate([up4_3, conv4_1, conv4_2], axis = 3)\n",
    "        conv4_3 = standard_uint(conv4_3, nb_filter = 128)\n",
    "        \n",
    "        up3_4 = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = 'same')(conv4_3)\n",
    "        conv3_4 = concatenate([up3_4, conv3_1, conv3_2, conv3_3], axis = 3)\n",
    "        conv3_4 = standard_uint(conv3_4, nb_filter = 64)\n",
    "        \n",
    "        up2_5 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_4)\n",
    "        conv2_5 = concatenate([up2_5, conv2_1, conv2_2, conv2_3, conv2_4], axis = 3)\n",
    "        conv2_5 = standard_uint(conv2_5, nb_filter = 32)   \n",
    "        \n",
    "        up1_6 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_5)\n",
    "        conv1_6 = concatenate([up1_6, conv1_1, conv1_2, conv1_3, conv1_4, conv1_5], axis = 3)\n",
    "        conv1_6 = standard_uint(conv1_6, nb_filter = 16)   \n",
    "        \n",
    "        # Center\n",
    "        conv7_1 = standard_uint(pool6, nb_filter = 1024)\n",
    "        \n",
    "        up6_2 = Conv2DTranspose(512, (2, 2), strides = (2, 2), padding = 'same')(conv7_1)\n",
    "        conv6_2 = concatenate([up6_2, conv6_1], axis = 3)\n",
    "        conv6_2 = standard_uint(conv6_2, nb_filter = 512)\n",
    "        \n",
    "        up5_3 = Conv2DTranspose(256, (2, 2), strides = (2, 2), padding = 'same')(conv6_2)\n",
    "        conv5_3 = concatenate([up5_3, conv5_1, conv5_2], axis = 3)\n",
    "        conv5_3 = standard_uint(conv5_3, nb_filter = 256)  \n",
    "        \n",
    "        up4_4 = Conv2DTranspose(128, (2, 2), strides = (2, 2), padding = 'same')(conv5_3)\n",
    "        conv4_4 = concatenate([up4_4, conv4_1, conv4_2, conv4_3], axis = 3)\n",
    "        conv4_4 = standard_uint(conv4_4, nb_filter = 128)     \n",
    "        \n",
    "        up3_5 = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = 'same')(conv4_4)\n",
    "        conv3_5 = concatenate([up3_5, conv3_1, conv3_2, conv3_3, conv3_4], axis = 3)\n",
    "        conv3_5 = standard_uint(conv3_5, nb_filter = 64)\n",
    "        \n",
    "        up2_6 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_5)\n",
    "        conv2_6 = concatenate([up2_6, conv2_1, conv2_2, conv2_3, conv2_4, conv2_5], axis = 3)\n",
    "        conv2_6 = standard_uint(conv2_6, nb_filter = 32)    \n",
    "        \n",
    "        up1_7 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_6)\n",
    "        conv1_7 = concatenate([up1_7, conv1_1, conv1_2, conv1_3, conv1_4, conv1_5, conv1_6], axis = 3)\n",
    "        conv1_7 = standard_uint(conv1_7, nb_filter = 16)\n",
    "        \n",
    "        \n",
    "        nested_output_1 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_2)\n",
    "        nested_output_2 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_3)\n",
    "        nested_output_3 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_4)\n",
    "        nested_output_4 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_5)\n",
    "        nested_output_5 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_6)\n",
    "        nested_output_6 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_7)\n",
    "        \n",
    "        if deep_supervision:\n",
    "            model = Model(inputs = inputs, outputs = [nested_output_1, nested_output_2, nested_output_3, nested_output_4, nested_output_5, nested_output_6]) \n",
    "        else:\n",
    "            model = Model(inputs = inputs, outputs = [nested_output_6])\n",
    "            \n",
    "        model.compile(optimizer=RMSprop(lr=0.0001), loss=bce_dice_loss, metrics=[dice_coeff])\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "def get_nest_unet_512_layer_5(input_shape=(512, 512, 1), num_classes=3, deep_supervision = False):\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        \n",
    "        inputs = Input(shape = input_shape)\n",
    "        \n",
    "        # 512\n",
    "        conv1_1 = standard_uint(inputs, nb_filter = 16)\n",
    "        pool1 = MaxPooling2D((2, 2), strides = (2, 2))(conv1_1)\n",
    "        \n",
    "        #256\n",
    "        conv2_1 = standard_uint(pool1, nb_filter = 32)\n",
    "        pool2 = MaxPooling2D((2, 2), strides = (2, 2))(conv2_1)\n",
    "        \n",
    "        up1_2 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_1)\n",
    "        conv1_2 = concatenate([up1_2, conv1_1], axis = 3)\n",
    "        conv1_2 = standard_uint(conv1_2, nb_filter = 16)\n",
    "        \n",
    "        #128\n",
    "        conv3_1 = standard_uint(pool2, nb_filter = 64)\n",
    "        pool3 = MaxPooling2D((2, 2), strides = (2, 2))(conv3_1)\n",
    "        \n",
    "        up2_2 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_1)\n",
    "        conv2_2 = concatenate([up2_2, conv2_1], axis = 3)\n",
    "        conv2_2 = standard_uint(conv2_2, nb_filter = 32)\n",
    "        \n",
    "        up1_3 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_2)\n",
    "        conv1_3 = concatenate([up1_3, conv1_1, conv1_2], axis = 3)\n",
    "        conv1_3 = standard_uint(conv1_3, nb_filter = 16)\n",
    "        \n",
    "        # 64\n",
    "        conv4_1 = standard_uint(pool3, nb_filter = 128)\n",
    "        pool4 = MaxPooling2D((2, 2), strides = (2, 2))(conv4_1)      \n",
    "        \n",
    "        up3_2 = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = 'same')(conv4_1)\n",
    "        conv3_2 = concatenate([up3_2, conv3_1], axis = 3)\n",
    "        conv3_2 = standard_uint(conv3_2, nb_filter = 64)    \n",
    "        \n",
    "        up2_3 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_2)\n",
    "        conv2_3 = concatenate([up2_3, conv2_1, conv2_2], axis = 3)\n",
    "        conv2_3 = standard_uint(conv2_3, nb_filter = 32)        \n",
    "        \n",
    "        up1_4 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_3)\n",
    "        conv1_4 = concatenate([up1_4, conv1_2, conv1_3], axis = 3)\n",
    "        conv1_4 = standard_uint(conv1_4, nb_filter = 16)       \n",
    "        \n",
    "        # 32\n",
    "        conv5_1 = standard_uint(pool4, nb_filter = 256)    \n",
    "        \n",
    "        up4_2 = Conv2DTranspose(128, (2, 2), strides = (2, 2), padding = 'same')(conv5_1)\n",
    "        conv4_2 = concatenate([up4_2, conv4_1], axis = 3)\n",
    "        conv4_2 = standard_uint(conv4_2, nb_filter = 128)  \n",
    "        \n",
    "        up3_3 = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = 'same')(conv4_2)\n",
    "        conv3_3 = concatenate([up3_3, conv3_1, conv3_2], axis = 3)\n",
    "        conv3_3 = standard_uint(conv3_3, nb_filter = 64)\n",
    "        \n",
    "        up2_4 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_3)\n",
    "        conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], axis = 3)\n",
    "        conv2_4 = standard_uint(conv2_4, nb_filter = 32)\n",
    "        \n",
    "        up1_5 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_4)\n",
    "        conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], axis = 3)\n",
    "        conv1_5 = standard_uint(conv1_5, nb_filter = 16)\n",
    "        \n",
    "        \n",
    "        \n",
    "        nested_output_1 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_2)\n",
    "        nested_output_2 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_3)\n",
    "        nested_output_3 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_4)\n",
    "        nested_output_4 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_5)\n",
    "        \n",
    "        if deep_supervision:\n",
    "            model = Model(inputs = inputs, outputs = [nested_output_1, nested_output_2, nested_output_3, nested_output_4]) \n",
    "        else:\n",
    "            model = Model(inputs = inputs, outputs = [nested_output_4])\n",
    "            \n",
    "        model.compile(optimizer=RMSprop(lr=0.0001), loss=bce_dice_loss, metrics=[dice_coeff])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_nest_unet_512_layer_5_more_channels(input_shape=(512, 512, 1), num_classes=3, deep_supervision = False):\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        \n",
    "        inputs = Input(shape = input_shape)\n",
    "        \n",
    "        # 512\n",
    "        conv1_1 = standard_uint(inputs, nb_filter = 16)\n",
    "        pool1 = MaxPooling2D((2, 2), strides = (2, 2))(conv1_1)\n",
    "        \n",
    "        #256\n",
    "        conv2_1 = standard_uint(pool1, nb_filter = 32)\n",
    "        pool2 = MaxPooling2D((2, 2), strides = (2, 2))(conv2_1)\n",
    "        \n",
    "        up1_2 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_1)\n",
    "        conv1_2 = concatenate([up1_2, conv1_1], axis = 3)\n",
    "        conv1_2 = standard_uint(conv1_2, nb_filter = 16)\n",
    "        \n",
    "        #128\n",
    "        conv3_1 = standard_uint(pool2, nb_filter = 64)\n",
    "        pool3 = MaxPooling2D((2, 2), strides = (2, 2))(conv3_1)\n",
    "        \n",
    "        up2_2 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_1)\n",
    "        conv2_2 = concatenate([up2_2, conv2_1], axis = 3)\n",
    "        conv2_2 = standard_uint(conv2_2, nb_filter = 32)\n",
    "        \n",
    "        up1_3 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_2)\n",
    "        conv1_3 = concatenate([up1_3, conv1_1, conv1_2], axis = 3)\n",
    "        conv1_3 = standard_uint(conv1_3, nb_filter = 16)\n",
    "        \n",
    "        # 64\n",
    "        conv4_1 = standard_uint(pool3, nb_filter = 128)\n",
    "        pool4 = MaxPooling2D((2, 2), strides = (2, 2))(conv4_1)      \n",
    "        \n",
    "        up3_2 = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = 'same')(conv4_1)\n",
    "        conv3_2 = concatenate([up3_2, conv3_1], axis = 3)\n",
    "        conv3_2 = standard_uint(conv3_2, nb_filter = 64)    \n",
    "        \n",
    "        up2_3 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_2)\n",
    "        conv2_3 = concatenate([up2_3, conv2_1, conv2_2], axis = 3)\n",
    "        conv2_3 = standard_uint(conv2_3, nb_filter = 32)        \n",
    "        \n",
    "        up1_4 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_3)\n",
    "        conv1_4 = concatenate([up1_4, conv1_2, conv1_3], axis = 3)\n",
    "        conv1_4 = standard_uint(conv1_4, nb_filter = 16)       \n",
    "        \n",
    "        # 32\n",
    "        conv5_1 = standard_uint(pool4, nb_filter = 256)    \n",
    "        \n",
    "        up4_2 = Conv2DTranspose(128, (2, 2), strides = (2, 2), padding = 'same')(conv5_1)\n",
    "        conv4_2 = concatenate([up4_2, conv4_1], axis = 3)\n",
    "        conv4_2 = standard_uint(conv4_2, nb_filter = 128)  \n",
    "        \n",
    "        up3_3 = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = 'same')(conv4_2)\n",
    "        conv3_3 = concatenate([up3_3, conv3_1, conv3_2], axis = 3)\n",
    "        conv3_3 = standard_uint(conv3_3, nb_filter = 64)\n",
    "        \n",
    "        up2_4 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same')(conv3_3)\n",
    "        conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3], axis = 3)\n",
    "        conv2_4 = standard_uint(conv2_4, nb_filter = 32)\n",
    "        \n",
    "        up1_5 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same')(conv2_4)\n",
    "        conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], axis = 3)\n",
    "        conv1_5 = standard_uint(conv1_5, nb_filter = 16)\n",
    "        \n",
    "        \n",
    "        \n",
    "        nested_output_1 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_2)\n",
    "        nested_output_2 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_3)\n",
    "        nested_output_3 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_4)\n",
    "        nested_output_4 = Conv2D(num_classes, (1, 1), activation = 'sigmoid')(conv1_5)\n",
    "        \n",
    "        if deep_supervision:\n",
    "            model = Model(inputs = inputs, outputs = [nested_output_1, nested_output_2, nested_output_3, nested_output_4]) \n",
    "        else:\n",
    "            model = Model(inputs = inputs, outputs = [nested_output_4])\n",
    "            \n",
    "        model.compile(optimizer=RMSprop(lr=0.0001), loss=bce_dice_loss, metrics=[dice_coeff])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e8a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def weighted_dice_coeff(y_true, y_pred, weight):\n",
    "    smooth = 1.\n",
    "    w, m1, m2 = weight * weight, y_true, y_pred\n",
    "    intersection = (m1 * m2)\n",
    "    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n",
    "    return score\n",
    "\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd number\n",
    "    if K.int_shape(y_pred)[1] == 128:\n",
    "        kernel_size = 11\n",
    "    elif K.int_shape(y_pred)[1] == 256:\n",
    "        kernel_size = 21\n",
    "    elif K.int_shape(y_pred)[1] == 512:\n",
    "        kernel_size = 21\n",
    "    elif K.int_shape(y_pred)[1] == 1024:\n",
    "        kernel_size = 41\n",
    "    else:\n",
    "        raise ValueError('Unexpected image size')\n",
    "    averaged_mask = K.pool2d(\n",
    "        y_true, pool_size=(kernel_size, kernel_size), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    border = K.cast(K.greater(averaged_mask, 0.005), 'float32') * K.cast(K.less(averaged_mask, 0.995), 'float32')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight += border * 2\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = 1 - weighted_dice_coeff(y_true, y_pred, weight)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight):\n",
    "    # avoiding overflow\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits\n",
    "    loss = (1. - y_true) * logit_y_pred + (1. + (weight - 1.) * y_true) * \\\n",
    "                                          (K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n",
    "    return K.sum(loss) / K.sum(weight)\n",
    "\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd number\n",
    "    if K.int_shape(y_pred)[1] == 128:\n",
    "        kernel_size = 11\n",
    "    elif K.int_shape(y_pred)[1] == 256:\n",
    "        kernel_size = 21\n",
    "    elif K.int_shape(y_pred)[1] == 512:\n",
    "        kernel_size = 21\n",
    "    elif K.int_shape(y_pred)[1] == 1024:\n",
    "        kernel_size = 41\n",
    "    else:\n",
    "        raise ValueError('Unexpected image size')\n",
    "    averaged_mask = K.pool2d(\n",
    "        y_true, pool_size=(kernel_size, kernel_size), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    border = K.cast(K.greater(averaged_mask, 0.005), 'float32') * K.cast(K.less(averaged_mask, 0.995), 'float32')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight += border * 2\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = weighted_bce_loss(y_true, y_pred, weight) + (1 - weighted_dice_coeff(y_true, y_pred, weight))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364ff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_margin(imgs):\n",
    "    \n",
    "    \"\"\"\n",
    "        412の倍数のH,Wにする\n",
    "    \"\"\"\n",
    "    \n",
    "    original_image_shape = imgs.shape\n",
    "    H = -(-original_image_shape[1]//412)\n",
    "    W = -(-original_image_shape[2]//412) \n",
    "    made_imgs = np.zeros(\n",
    "        (original_image_shape[0], H*412, W*412, 1), \n",
    "        np.float32\n",
    "    )\n",
    "    \n",
    "    start_h = (H*412 - original_image_shape[1]) // 2\n",
    "    start_w = (W*412 - original_image_shape[2]) // 2\n",
    "    \n",
    "    made_imgs[:, start_h:start_h+original_image_shape[1], start_w:start_w+original_image_shape[2], :] = imgs\n",
    "    return made_imgs\n",
    "\n",
    "\n",
    "def delete_margin(imgs, original_image_shape):\n",
    "    \n",
    "    \"\"\"\n",
    "        marginを削除\n",
    "    \"\"\"\n",
    "    \n",
    "    H = -(-original_image_shape[1]//412)\n",
    "    W = -(-original_image_shape[2]//412)\n",
    "    \n",
    "    start_h = (H*412 - original_image_shape[1]) // 2\n",
    "    start_w = (W*412 - original_image_shape[2]) // 2\n",
    "    \n",
    "    return imgs[:, start_h:start_h+original_image_shape[1], start_w:start_w+original_image_shape[2], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162cea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習後のU-Netによる予測を行う関数\n",
    "def predict(X_test, model_path, filenames, out_dir, input_shape=(512, 512, 1), num_classes=1):\n",
    "\n",
    "    # testDataフォルダ配下にleft_imagesフォルダを置いている\n",
    "    \n",
    "    model = get_nest_unet_512_layer_5(input_shape=input_shape, num_classes=num_classes)\n",
    "    \n",
    "    model.load_weights(model_path)\n",
    "    BATCH_SIZE = 1\n",
    "    Y_pred = model.predict(X_test, BATCH_SIZE)\n",
    "    \n",
    "    print(Y_pred.shape)\n",
    "    os.makedirs(out_dir, exist_ok = True)\n",
    "    \n",
    "    if Y_pred.shape[3]!=1:\n",
    "        num = Y_pred.shape[3]\n",
    "        for n in range(num):\n",
    "            os.makedirs(os.path.join(out_dir,str(n+1)), exist_ok=True)\n",
    "        for i, y in enumerate(Y_pred):\n",
    "            for n in range(num):\n",
    "                os.makedirs(os.path.join(out_dir,str(n+1)), exist_ok=True)\n",
    "            # testDataフォルダ配下にleft_imagesフォルダを置いている\n",
    "            #img = cv2.imread('512/hira/For_Seika/learning/original/'+ file_names[i], cv2.IMREAD_GRAYSCALE)\n",
    "            #y = cv2.resize(y, (img.shape[1], img.shape[0]))\n",
    "                cv2.imwrite(os.path.join(out_dir, str(n+1) , str(i).zfill(6) + '.png'), denormalize_y(y[:,:,n]))\n",
    "        \n",
    "    else:\n",
    "        for i, y in enumerate(Y_pred):\n",
    "            # testDataフォルダ配下にleft_imagesフォルダを置いている\n",
    "            #img = cv2.imread('512/hira/For_Seika/learning/original/'+ file_names[i], cv2.IMREAD_GRAYSCALE)\n",
    "            #y = cv2.resize(y, (img.shape[1], img.shape[0]))\n",
    "            cv2.imwrite(os.path.join(out_dir , str(i).zfill(6) + '.png'), denormalize_y(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a823492f",
   "metadata": {},
   "source": [
    "# shCtrl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b6c18",
   "metadata": {},
   "source": [
    "## cropped_001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e0993",
   "metadata": {},
   "source": [
    "### pred_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1189133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_001/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be5304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_001/mito_active_learning/mito_nested_unet_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_xy_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_yz_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_zx_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1436cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_xy_002')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_yz_002')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_zx_002')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2654e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/merged_mito_002'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839503df",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/modified_mito_002'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d930f1",
   "metadata": {},
   "source": [
    "### pred_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_001/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86345a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_001/mito_active_learning/mito_nested_unet_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fe189",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_xy_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_yz_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_zx_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb7018",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_xy_003')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_yz_003')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_zx_003')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d89af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/merged_mito_003'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21db5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/modified_mito_003'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ee793",
   "metadata": {},
   "source": [
    "### pred_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_001/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_001/mito_active_learning/mito_nested_unet_003_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b65c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_xy_004/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_yz_004/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_zx_004/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_xy_004')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_yz_004')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_zx_004')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/merged_mito_004'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62540ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/modified_mito_004'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e6ac61",
   "metadata": {},
   "source": [
    "### pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58574102",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_001/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15147b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_xy_004')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_yz_004')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/pred_mito_zx_004')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becbc14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_001/merged_mito_final'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(mito_imgs_ave.shape[0]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da62c86",
   "metadata": {},
   "source": [
    "## cropped_002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07250ab",
   "metadata": {},
   "source": [
    "### pred_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_002/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9617a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_002/mito_active_learning/mito_nested_unet_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8095223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_xy_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_yz_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_zx_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6aa0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_xy_002')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_yz_002')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_zx_002')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/merged_mito_002'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac9de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/modified_mito_002'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d77b05",
   "metadata": {},
   "source": [
    "### pred_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a733ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_002/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_002/mito_active_learning/mito_nested_unet_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab72002",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_xy_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_yz_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_zx_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be257cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_xy_003')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_yz_003')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_zx_003')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/merged_mito_003'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151a975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/modified_mito_003'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b95ecf9",
   "metadata": {},
   "source": [
    "### pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc42730",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_002/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_xy_003')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_yz_003')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/pred_mito_zx_003')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a463eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_002/merged_mito_final'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(mito_imgs_ave.shape[0]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4098e",
   "metadata": {},
   "source": [
    "## cropped_003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67f380",
   "metadata": {},
   "source": [
    "### pred_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac180ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_003/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c945d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_003/mito_active_learning/mito_nested_unet_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60534dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_xy_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_yz_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_zx_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706e57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_xy_002')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_yz_002')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_zx_002')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/merged_mito_002'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d24586",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/modified_mito_002'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8471cd",
   "metadata": {},
   "source": [
    "### pred_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_003/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b7d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_003/mito_active_learning/mito_nested_unet_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dec99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_xy_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_yz_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_zx_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed997d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_xy_003')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_yz_003')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_zx_003')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da893e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/merged_mito_003'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f87a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/modified_mito_003'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0568bf7",
   "metadata": {},
   "source": [
    "### pred_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_003/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bcbc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_003/mito_active_learning/mito_nested_unet_003_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d04e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_xy_004/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_yz_004/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_zx_004/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc881fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_xy_004')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_yz_004')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_zx_004')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aec46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/merged_mito_004'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4d66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/modified_mito_004'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56f73d3",
   "metadata": {},
   "source": [
    "### pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46902aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_003/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b4af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_xy_004')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_yz_004')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/pred_mito_zx_004')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb9f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_003/merged_mito_final'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(mito_imgs_ave.shape[0]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b729bd",
   "metadata": {},
   "source": [
    "## cropped_004"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43a65d",
   "metadata": {},
   "source": [
    "### pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/mito_active_learning/mito_nested_unet_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_004/resize_10x10x10/')\n",
    "ori_image_shape = ori_imgs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb7380",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_xy_imgs.shape[0]):\n",
    "    if np.size(seped_xy_imgs[i]) != np.sum(seped_xy_imgs[i] == -1):\n",
    "        target_imgs.append(seped_xy_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_004/pred_mito_xy_001/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_yz_imgs.shape[0]):\n",
    "    if np.size(seped_yz_imgs[i]) != np.sum(seped_yz_imgs[i] == -1):\n",
    "        target_imgs.append(seped_yz_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_004/pred_mito_yz_001/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test, target_imgs, target_imgs_lst\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_zx_imgs.shape[0]):\n",
    "    if np.size(seped_zx_imgs[i]) != np.sum(seped_zx_imgs[i] == -1):\n",
    "        target_imgs.append(seped_zx_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_004/pred_mito_zx_001/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test, target_imgs, target_imgs_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64246b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_xy, imgs_xy_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_004/pred_mito_xy_001')\n",
    "\n",
    "full_imgs_xy_sheets = ori_image_shape[0] * (-(-ori_image_shape[1] // 412)) * (-(-ori_image_shape[2] // 412))\n",
    "full_imgs_xy = np.zeros((full_imgs_xy_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_xy, imgs_xy_filename):\n",
    "    full_imgs_xy[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_xy = merge_imgs(full_imgs_xy, ori_image_shape)\n",
    "\n",
    "del imgs_xy, full_imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, imgs_yz_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_004/pred_mito_yz_001')\n",
    "\n",
    "full_imgs_yz_sheets = ori_image_shape[2] * (-(-ori_image_shape[0] // 412)) * (-(-ori_image_shape[1] // 412))\n",
    "full_imgs_yz = np.zeros((full_imgs_yz_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_yz, imgs_yz_filename):\n",
    "    full_imgs_yz[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_yz = merge_imgs(full_imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "\n",
    "del imgs_yz, full_imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, imgs_zx_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_004/pred_mito_zx_001')\n",
    "\n",
    "full_imgs_zx_sheets = ori_image_shape[1] * (-(-ori_image_shape[2] // 412)) * (-(-ori_image_shape[0] // 412))\n",
    "full_imgs_zx = np.zeros((full_imgs_zx_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_zx, imgs_zx_filename):\n",
    "    full_imgs_zx[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_zx = merge_imgs(full_imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "\n",
    "del imgs_zx, full_imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88071a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_004/modified_mito'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(imgs_ave.shape[0]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.where(\n",
    "        imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    cnt += 1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a544934",
   "metadata": {},
   "source": [
    "## cropped_005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296a8b1",
   "metadata": {},
   "source": [
    "### pred_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c594b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z://DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_005/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ee2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 初回だけ\n",
    "\n",
    "# save_ori_imgs\n",
    "ori_image_shape = ori_imgs.shape\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/original'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    img = ori_imgs[i][y_range[0]:y_range[1], x_range[0]:x_range[1], :]\n",
    "    cv2.imwrite(\n",
    "        f\"{out_dir}/{str(cnt).zfill(4)}.png\",\n",
    "        denormalize_x(img)\n",
    "    )\n",
    "    cnt += 1\n",
    "    \n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/mito_active_learning/mito_nested_unet_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fd08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/pred_mito_xy_001/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/pred_mito_yz_001/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/pred_mito_zx_001/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "ori_image_shape\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/pred_mito_xy_001')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/pred_mito_yz_001')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/pred_mito_zx_001')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a53baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/merged_mito_001'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feef99e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_005/modified_mito_001'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c027fd",
   "metadata": {},
   "source": [
    "### pred_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c7b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z://DeepLearningData/research_010_NIH3T3/shCtrl_003/dataset/cropped_005/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411eb324",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_005/mito_active_learning/mito_nested_unet_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d62f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:DeepLearningData/research_010_NIH3T3/shCtrl_003/models/cropped_005/mito_active_learning/mito_nested_unet_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "ori_image_shape\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/pred_mito_xy_002')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/pred_mito_yz_002')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/pred_mito_zx_002')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/merged_mito_002'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotatioins/cropped_005/modified_mito_002'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0aa086",
   "metadata": {},
   "source": [
    "### pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9efd86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shCtrl_003/annotations/cropped_005/modified_mito'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(imgs_ave.shape[0]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.where(\n",
    "        imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    cnt += 1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d7f67a",
   "metadata": {},
   "source": [
    "# shOPA1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3e68bc",
   "metadata": {},
   "source": [
    "## cropped_001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7768a36",
   "metadata": {},
   "source": [
    "### pred_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z://DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_001/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/mito_active_learning/mito_nested_unet_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f47c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_xy_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_yz_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_zx_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "ori_image_shape\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_xy_002')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_yz_002')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_zx_002')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebeb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/merged_mito_002'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/modified_mito_002'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e2c984",
   "metadata": {},
   "source": [
    "### pred_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z://DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_001/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9871b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/mito_active_learning/mito_nested_unet_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_xy_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_yz_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_zx_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b834fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "ori_image_shape\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_xy_003')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_yz_003')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_zx_003')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d53345",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/merged_mito_003'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ddfb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/modified_mito_003'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f7b77",
   "metadata": {},
   "source": [
    "### pred_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d4992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z://DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_001/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cfd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/mito_active_learning/mito_nested_unet_003_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81dd231",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_xy_004/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_yz_004/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_zx_004/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "ori_image_shape\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_xy_004')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_yz_004')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_zx_004')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79613f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/merged_mito_004'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/modified_mito_004'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68cf593",
   "metadata": {},
   "source": [
    "### pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d279ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z://DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_001/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b4a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "ori_image_shape\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_xy_004')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_yz_004')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/pred_mito_zx_004')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6df7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_001/merged_mito_final'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(mito_imgs_ave.shape[0]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942abf22",
   "metadata": {},
   "source": [
    "## cropped_002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d5b84e",
   "metadata": {},
   "source": [
    "### pred_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70551af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z://DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_002/resize_10x10x10/')\n",
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_002/mito_active_learning/mito_nested_unet_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b98ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_xy_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_yz_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_zx_002/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "ori_image_shape\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_xy_002')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_yz_002')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_zx_002')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb42429",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/merged_mito_002'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3eb476",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/modified_mito_002'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d466c2b6",
   "metadata": {},
   "source": [
    "### pred_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b67c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z://DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_002/resize_10x10x10/')\n",
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_002/mito_active_learning/mito_nested_unet_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b631c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_xy_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_yz_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_zx_003/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "ori_image_shape\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_xy_003')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_yz_003')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_zx_003')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/merged_mito_003'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86def110",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/modified_mito_003'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d603ce8",
   "metadata": {},
   "source": [
    "### pred_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6824ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z://DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_002/resize_10x10x10/')\n",
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_002/mito_active_learning/mito_nested_unet_003_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e96dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "X_test = seped_xy_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_xy_004/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "X_test = seped_yz_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_yz_004/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "X_test = seped_zx_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_zx_004/'\n",
    "predict(X_test, model_path, filenames, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f3dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "ori_image_shape\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_xy_004')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_yz_004')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_zx_004')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/merged_mito_004'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        mito_imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(6)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/modified_mito_004'\n",
    "os.makedirs(path, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c85b30",
   "metadata": {},
   "source": [
    "### pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9dac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z://DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_002/resize_10x10x10/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d97d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_image_shape = ori_imgs.shape\n",
    "del ori_imgs\n",
    "ori_image_shape\n",
    "\n",
    "\n",
    "imgs_xy, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_xy_004')\n",
    "merged_imgs_xy = merge_imgs(imgs_xy, ori_image_shape)\n",
    "del imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_yz_004')\n",
    "merged_imgs_yz = merge_imgs(imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "del imgs_yz\n",
    "\n",
    "\n",
    "\n",
    "imgs_zx, _ = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/pred_mito_zx_004')\n",
    "merged_imgs_zx = merge_imgs(imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "del imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c01e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "mito_imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotatioins/cropped_002/merged_mito_final'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(mito_imgs_ave.shape[0]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.where(\n",
    "        mito_imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c47c0e1",
   "metadata": {},
   "source": [
    "## cropped_003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63531ee1",
   "metadata": {},
   "source": [
    "### pred_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de8e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_003/mito_active_learning/mito_nested_unet_001_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c982a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_003/resize_10x10x10/')\n",
    "ori_image_shape = ori_imgs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_xy_imgs.shape[0]):\n",
    "    if np.size(seped_xy_imgs[i]) != np.sum(seped_xy_imgs[i] == -1):\n",
    "        target_imgs.append(seped_xy_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/pred_mito_xy_002/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_yz_imgs.shape[0]):\n",
    "    if np.size(seped_yz_imgs[i]) != np.sum(seped_yz_imgs[i] == -1):\n",
    "        target_imgs.append(seped_yz_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/pred_mito_yz_002/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test, target_imgs, target_imgs_lst\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_zx_imgs.shape[0]):\n",
    "    if np.size(seped_zx_imgs[i]) != np.sum(seped_zx_imgs[i] == -1):\n",
    "        target_imgs.append(seped_zx_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/pred_mito_zx_002/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test, target_imgs, target_imgs_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0786582",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_xy, imgs_xy_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/pred_mito_xy_002')\n",
    "\n",
    "full_imgs_xy_sheets = ori_image_shape[0] * (-(-ori_image_shape[1] // 412)) * (-(-ori_image_shape[2] // 412))\n",
    "full_imgs_xy = np.zeros((full_imgs_xy_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_xy, imgs_xy_filename):\n",
    "    full_imgs_xy[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_xy = merge_imgs(full_imgs_xy, ori_image_shape)\n",
    "\n",
    "del imgs_xy, full_imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, imgs_yz_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/pred_mito_yz_002')\n",
    "\n",
    "full_imgs_yz_sheets = ori_image_shape[2] * (-(-ori_image_shape[0] // 412)) * (-(-ori_image_shape[1] // 412))\n",
    "full_imgs_yz = np.zeros((full_imgs_yz_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_yz, imgs_yz_filename):\n",
    "    full_imgs_yz[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_yz = merge_imgs(full_imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "\n",
    "del imgs_yz, full_imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, imgs_zx_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/pred_mito_zx_002')\n",
    "\n",
    "full_imgs_zx_sheets = ori_image_shape[1] * (-(-ori_image_shape[2] // 412)) * (-(-ori_image_shape[0] // 412))\n",
    "full_imgs_zx = np.zeros((full_imgs_zx_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_zx, imgs_zx_filename):\n",
    "    full_imgs_zx[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_zx = merge_imgs(full_imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "\n",
    "del imgs_zx, full_imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ea844",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/merged_mito_002'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        imgs_ave[:,:,:,0][i] >= 127,\n",
    "        imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(4)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/manually_mito_002/'\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c1e388",
   "metadata": {},
   "source": [
    "### pred_002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35fba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_003/mito_active_learning/mito_nested_unet_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_imgs, filename =  load_X_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/dataset/cropped_003/resize_10x10x10/')\n",
    "ori_image_shape = ori_imgs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_xy_imgs.shape[0]):\n",
    "    if np.size(seped_xy_imgs[i]) != np.sum(seped_xy_imgs[i] == -1):\n",
    "        target_imgs.append(seped_xy_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/pred_mito_xy_003/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_yz_imgs.shape[0]):\n",
    "    if np.size(seped_yz_imgs[i]) != np.sum(seped_yz_imgs[i] == -1):\n",
    "        target_imgs.append(seped_yz_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/pred_mito_yz_003/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test, target_imgs, target_imgs_lst\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_zx_imgs.shape[0]):\n",
    "    if np.size(seped_zx_imgs[i]) != np.sum(seped_zx_imgs[i] == -1):\n",
    "        target_imgs.append(seped_zx_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/pred_mito_zx_003/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test, target_imgs, target_imgs_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1213ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_xy, imgs_xy_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/pred_mito_xy_003')\n",
    "\n",
    "full_imgs_xy_sheets = ori_image_shape[0] * (-(-ori_image_shape[1] // 412)) * (-(-ori_image_shape[2] // 412))\n",
    "full_imgs_xy = np.zeros((full_imgs_xy_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_xy, imgs_xy_filename):\n",
    "    full_imgs_xy[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_xy = merge_imgs(full_imgs_xy, ori_image_shape)\n",
    "\n",
    "del imgs_xy, full_imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, imgs_yz_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/pred_mito_yz_003')\n",
    "\n",
    "full_imgs_yz_sheets = ori_image_shape[2] * (-(-ori_image_shape[0] // 412)) * (-(-ori_image_shape[1] // 412))\n",
    "full_imgs_yz = np.zeros((full_imgs_yz_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_yz, imgs_yz_filename):\n",
    "    full_imgs_yz[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_yz = merge_imgs(full_imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "\n",
    "del imgs_yz, full_imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, imgs_zx_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/pred_mito_zx_003')\n",
    "\n",
    "full_imgs_zx_sheets = ori_image_shape[1] * (-(-ori_image_shape[2] // 412)) * (-(-ori_image_shape[0] // 412))\n",
    "full_imgs_zx = np.zeros((full_imgs_zx_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_zx, imgs_zx_filename):\n",
    "    full_imgs_zx[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_zx = merge_imgs(full_imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "\n",
    "del imgs_zx, full_imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/merged_mito_003'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        imgs_ave[:,:,:,0][i] >= 127,\n",
    "        imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(4)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7806bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/manually_mito_003/'\n",
    "os.makedirs(out_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838c473",
   "metadata": {},
   "source": [
    "### pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_003/merged_mito_final'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(imgs_ave.shape[0]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.where(\n",
    "        imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "\n",
    "    cnt += 1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c498d3d",
   "metadata": {},
   "source": [
    "## cropped_004"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e40df2",
   "metadata": {},
   "source": [
    "### pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/mito_active_learning/mito_nested_unet_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e6d242",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_xy_imgs.shape[0]):\n",
    "    if np.size(seped_xy_imgs[i]) != np.sum(seped_xy_imgs[i] == -1):\n",
    "        target_imgs.append(seped_xy_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_004/pred_mito_xy_002/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_yz_imgs.shape[0]):\n",
    "    if np.size(seped_yz_imgs[i]) != np.sum(seped_yz_imgs[i] == -1):\n",
    "        target_imgs.append(seped_yz_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_004/pred_mito_yz_002/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test, target_imgs, target_imgs_lst\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_zx_imgs.shape[0]):\n",
    "    if np.size(seped_zx_imgs[i]) != np.sum(seped_zx_imgs[i] == -1):\n",
    "        target_imgs.append(seped_zx_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_004/pred_mito_zx_002/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test, target_imgs, target_imgs_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fad1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_xy, imgs_xy_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_004/pred_mito_xy_002')\n",
    "\n",
    "full_imgs_xy_sheets = ori_image_shape[0] * (-(-ori_image_shape[1] // 412)) * (-(-ori_image_shape[2] // 412))\n",
    "full_imgs_xy = np.zeros((full_imgs_xy_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_xy, imgs_xy_filename):\n",
    "    full_imgs_xy[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_xy = merge_imgs(full_imgs_xy, ori_image_shape)\n",
    "\n",
    "del imgs_xy, full_imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, imgs_yz_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_004/pred_mito_yz_002')\n",
    "\n",
    "full_imgs_yz_sheets = ori_image_shape[2] * (-(-ori_image_shape[0] // 412)) * (-(-ori_image_shape[1] // 412))\n",
    "full_imgs_yz = np.zeros((full_imgs_yz_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_yz, imgs_yz_filename):\n",
    "    full_imgs_yz[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_yz = merge_imgs(full_imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "\n",
    "del imgs_yz, full_imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, imgs_zx_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_004/pred_mito_zx_002')\n",
    "\n",
    "full_imgs_zx_sheets = ori_image_shape[1] * (-(-ori_image_shape[2] // 412)) * (-(-ori_image_shape[0] // 412))\n",
    "full_imgs_zx = np.zeros((full_imgs_zx_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_zx, imgs_zx_filename):\n",
    "    full_imgs_zx[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_zx = merge_imgs(full_imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "\n",
    "del imgs_zx, full_imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2ce52",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_004/merged_mito_002'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        imgs_ave[:,:,:,0][i] >= 127,\n",
    "        imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(4)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b07c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_004/merged_mito_final'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(imgs_ave.shape[0]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.where(\n",
    "        imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "\n",
    "    cnt += 1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa96c44",
   "metadata": {},
   "source": [
    "## cropped_005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d610a6",
   "metadata": {},
   "source": [
    "### pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2274a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/models/cropped_001/mito_active_learning/mito_nested_unet_002_weights.hdf5'\n",
    "input_shape=(512, 512, 1)\n",
    "num_classes=1\n",
    "filenames = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af91fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seped_xy_imgs = divide_imgs(ori_imgs)\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_xy_imgs.shape[0]):\n",
    "    if np.size(seped_xy_imgs[i]) != np.sum(seped_xy_imgs[i] == -1):\n",
    "        target_imgs.append(seped_xy_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/pred_mito_xy_002/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_xy_imgs, X_test\n",
    "\n",
    "\n",
    "\n",
    "seped_yz_imgs = divide_imgs(ori_imgs.transpose(2, 0, 1, 3))\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_yz_imgs.shape[0]):\n",
    "    if np.size(seped_yz_imgs[i]) != np.sum(seped_yz_imgs[i] == -1):\n",
    "        target_imgs.append(seped_yz_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/pred_mito_yz_002/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_yz_imgs, X_test, target_imgs, target_imgs_lst\n",
    "\n",
    "\n",
    "seped_zx_imgs = divide_imgs(ori_imgs.transpose(1, 2, 0, 3))\n",
    "\n",
    "target_imgs = []\n",
    "target_imgs_lst = []\n",
    "for i in range(seped_zx_imgs.shape[0]):\n",
    "    if np.size(seped_zx_imgs[i]) != np.sum(seped_zx_imgs[i] == -1):\n",
    "        target_imgs.append(seped_zx_imgs[i])\n",
    "        target_imgs_lst.append(str(i).zfill(6))\n",
    "target_imgs = np.array(target_imgs)\n",
    "\n",
    "X_test = target_imgs\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/pred_mito_zx_002/'\n",
    "predict_with_name(X_test, model_path, target_imgs_lst, out_dir, input_shape, num_classes)\n",
    "\n",
    "del seped_zx_imgs, X_test, target_imgs, target_imgs_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e155ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_xy, imgs_xy_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/pred_mito_xy_002')\n",
    "\n",
    "full_imgs_xy_sheets = ori_image_shape[0] * (-(-ori_image_shape[1] // 412)) * (-(-ori_image_shape[2] // 412))\n",
    "full_imgs_xy = np.zeros((full_imgs_xy_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_xy, imgs_xy_filename):\n",
    "    full_imgs_xy[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_xy = merge_imgs(full_imgs_xy, ori_image_shape)\n",
    "\n",
    "del imgs_xy, full_imgs_xy\n",
    "\n",
    "\n",
    "imgs_yz, imgs_yz_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/pred_mito_yz_002')\n",
    "\n",
    "full_imgs_yz_sheets = ori_image_shape[2] * (-(-ori_image_shape[0] // 412)) * (-(-ori_image_shape[1] // 412))\n",
    "full_imgs_yz = np.zeros((full_imgs_yz_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_yz, imgs_yz_filename):\n",
    "    full_imgs_yz[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_yz = merge_imgs(full_imgs_yz, (ori_image_shape[2], ori_image_shape[0], ori_image_shape[1], ori_image_shape[3]))\n",
    "\n",
    "del imgs_yz, full_imgs_yz\n",
    "\n",
    "\n",
    "imgs_zx, imgs_zx_filename = load_Y_gray('Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/pred_mito_zx_002')\n",
    "\n",
    "full_imgs_zx_sheets = ori_image_shape[1] * (-(-ori_image_shape[2] // 412)) * (-(-ori_image_shape[0] // 412))\n",
    "full_imgs_zx = np.zeros((full_imgs_zx_sheets, 512, 512, 1), np.float32)\n",
    "for img, imgname in zip(imgs_zx, imgs_zx_filename):\n",
    "    full_imgs_zx[int(imgname.split(\".\")[0])] = img\n",
    "\n",
    "merged_imgs_zx = merge_imgs(full_imgs_zx, (ori_image_shape[1], ori_image_shape[2], ori_image_shape[0], ori_image_shape[3]))\n",
    "\n",
    "del imgs_zx, full_imgs_zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0274ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/merged_mito_002'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "os.makedirs(f\"{out_dir}_raw\", exist_ok = True)\n",
    "\n",
    "z_range = [int(ori_image_shape[0] * 0.1), int(ori_image_shape[0] * 0.9)]\n",
    "y_range = [int(ori_image_shape[1] * 0.1), int(ori_image_shape[1] * 0.9)]\n",
    "x_range = [int(ori_image_shape[2] * 0.1), int(ori_image_shape[2] * 0.9)]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for i in range(z_range[0], z_range[1]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img = np.where(\n",
    "        imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "    # averaged\n",
    "    img_ = np.zeros(\n",
    "        (y_range[1] - y_range[0], x_range[1] - x_range[0], 1), np.float32\n",
    "    )\n",
    "    img_ = np.where(\n",
    "        imgs_ave[:,:,:,0][i] >= 127,\n",
    "        imgs_ave[:,:,:,0][i],\n",
    "        0\n",
    "    )[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "    cv2.imwrite(f'{out_dir}_raw/{str(cnt).zfill(4)}.png', img_)\n",
    "    \n",
    "    cnt += 1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99901e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_ave = merged_imgs_xy * 255 // 3 + merged_imgs_yz.transpose(1, 2, 0, 3) * 255 // 3 + merged_imgs_zx.transpose(2, 0, 1, 3) * 255 //3\n",
    "\n",
    "\n",
    "out_dir = 'Z:/DeepLearningData/research_010_NIH3T3/shOPA1_003/annotations/cropped_005/merged_mito_final'\n",
    "os.makedirs(out_dir, exist_ok = True)\n",
    "\n",
    "cnt = 0\n",
    "for i in range(imgs_ave.shape[0]):\n",
    "    \n",
    "    # threshed\n",
    "    img = np.where(\n",
    "        imgs_ave[:,:,:,0][i] >= 127,\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "    cv2.imwrite(f'{out_dir}/{str(cnt).zfill(4)}.png', img)\n",
    "    \n",
    "\n",
    "    cnt += 1\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
